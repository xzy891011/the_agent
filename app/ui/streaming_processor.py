"""
LangGraphæµå¼å¤„ç†å™¨
è´Ÿè´£ç›‘å¬Graphæ‰§è¡Œè¿‡ç¨‹ä¸­çš„å„ç§äº‹ä»¶ï¼Œè½¬æ¢ä¸ºç»Ÿä¸€çš„æµå¼æ¶ˆæ¯æ ¼å¼

åŸºäºLangGraphå®˜æ–¹æ–‡æ¡£ä¼˜åŒ–: https://langchain-ai.github.io/langgraph/how-tos/streaming/
æ”¯æŒçš„æµæ¨¡å¼:
- messages: æµå¼ä¼ è¾“2å…ƒç»„(LLM token, metadata)
- updates: æµå¼ä¼ è¾“æ¯æ­¥ä¹‹åçš„çŠ¶æ€æ›´æ–°
- custom: æµå¼ä¼ è¾“è‡ªå®šä¹‰æ•°æ®
- values: æµå¼ä¼ è¾“å®Œæ•´çŠ¶æ€å€¼ï¼ˆé€šå¸¸ä¸æ¨é€åˆ°å‰ç«¯ï¼‰
- debug: æµå¼ä¼ è¾“è°ƒè¯•ä¿¡æ¯
"""

import logging
import time
from typing import Dict, Any, Optional, Generator, List, Callable, Union
from datetime import datetime
import asyncio
import json

from langchain_core.messages import BaseMessage, AIMessage, HumanMessage, ToolMessage

from app.ui.streaming_types import (
    StreamMessageType, BaseStreamMessage, NodeStatusMessage, RouterMessage,
    LLMTokenMessage, ToolExecutionMessage, FileGeneratedMessage, 
    AgentThinkingMessage, ErrorMessage, InfoMessage,
    create_message, serialize_message
)

logger = logging.getLogger(__name__)


class LangGraphStreamingProcessor:
    """LangGraphæµå¼å¤„ç†å™¨ - åŸºäºå®˜æ–¹æ–‡æ¡£ä¼˜åŒ–"""
    
    def __init__(self, session_id: Optional[str] = None):
        """
        åˆå§‹åŒ–æµå¼å¤„ç†å™¨
        
        Args:
            session_id: ä¼šè¯ID
        """
        self.session_id = session_id
        self.active_nodes: Dict[str, float] = {}  # èŠ‚ç‚¹å -> å¼€å§‹æ—¶é—´
        self.message_buffer: List[BaseStreamMessage] = []
        
        # äº‹ä»¶å¤„ç†å™¨æ˜ å°„
        self.event_handlers = {
            "on_chain_start": self._handle_node_start,
            "on_chain_end": self._handle_node_end,
            "on_chain_error": self._handle_node_error,
            "on_llm_start": self._handle_llm_start,
            "on_llm_new_token": self._handle_llm_token,
            "on_llm_end": self._handle_llm_end,
            "on_tool_start": self._handle_tool_start,
            "on_tool_end": self._handle_tool_end,
            "on_tool_error": self._handle_tool_error,
        }
        
        logger.info(f"LangGraphæµå¼å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä¼šè¯ID: {session_id}")
    
    def process_langgraph_stream(
        self, 
        stream_generator: Generator,
        stream_modes: List[str] = None
    ) -> Generator[Dict[str, Any], None, None]:
        """
        å¤„ç†LangGraphæµå¼è¾“å‡º
        
        Args:
            stream_generator: LangGraphæµç”Ÿæˆå™¨
            stream_modes: æµæ¨¡å¼åˆ—è¡¨
            
        Yields:
            å¤„ç†åçš„æµå¼æ¶ˆæ¯
        """
        if stream_modes is None:
            stream_modes = ["messages", "custom", "updates"]  # é»˜è®¤ä¸åŒ…å«valuesä»¥é¿å…å†—ä½™æ•°æ®
        
        logger.info(f"å¼€å§‹å¤„ç†LangGraphæµï¼Œæ¨¡å¼: {stream_modes}")
        
        try:
            for chunk in stream_generator:
                # å¤„ç†ä¸åŒç±»å‹çš„æµæ•°æ®
                messages = self._process_stream_chunk(chunk)
                
                # äº§ç”Ÿæ‰€æœ‰ç”Ÿæˆçš„æ¶ˆæ¯
                for message in messages:
                    yield serialize_message(message)
                    
        except Exception as e:
            logger.error(f"å¤„ç†LangGraphæµæ—¶å‡ºé”™: {str(e)}")
            error_msg = create_message(
                StreamMessageType.ERROR,
                session_id=self.session_id,
                source="streaming_processor",
                error_message=f"æµå¤„ç†é”™è¯¯: {str(e)}",
                error_code="STREAM_PROCESSING_ERROR"
            )
            yield serialize_message(error_msg)
    
    def _process_stream_chunk(self, chunk: Any) -> List[BaseStreamMessage]:
        """
        å¤„ç†å•ä¸ªæµæ•°æ®å— - åŸºäºçœŸå®æ ¼å¼ä¼˜åŒ–
        
        Args:
            chunk: LangGraphæµæ•°æ®å—
            
        Returns:
            ç”Ÿæˆçš„æ¶ˆæ¯åˆ—è¡¨
        """
        messages = []
        
        try:
            # æ·»åŠ è°ƒè¯•æ—¥å¿—æ¥æŸ¥çœ‹chunkæ ¼å¼
            logger.debug(f"[DEBUG] æ”¶åˆ°chunkç±»å‹: {type(chunk)}, å†…å®¹é¢„è§ˆ: {str(chunk)[:200]}...")
            
            # åŸºäºLangGraphå®˜æ–¹æ–‡æ¡£çš„chunkç±»å‹åˆ¤æ–­
            if self._is_message_chunk(chunk):
                logger.debug(f"[DEBUG] è¯†åˆ«ä¸ºmessagesæµchunk")
                messages.extend(self._handle_message_chunk(chunk))
            
            elif self._is_update_chunk(chunk):
                logger.debug(f"[DEBUG] è¯†åˆ«ä¸ºupdatesæµchunk")
                messages.extend(self._handle_update_chunk(chunk))
            
            elif self._is_custom_chunk(chunk):
                logger.debug(f"[DEBUG] è¯†åˆ«ä¸ºcustomæµchunk")
                messages.extend(self._handle_custom_chunk(chunk))
            
            elif self._is_values_chunk(chunk):
                logger.debug(f"[DEBUG] è¯†åˆ«ä¸ºvaluesæµchunkï¼ˆé€šå¸¸è·³è¿‡å¤„ç†ï¼‰")
                # valuesæµåŒ…å«å®Œæ•´çŠ¶æ€ï¼Œé€šå¸¸ä¸éœ€è¦æ¨é€åˆ°å‰ç«¯
                # åªåœ¨éœ€è¦ç‰¹å®šçŠ¶æ€ä¿¡æ¯æ—¶æ‰å¤„ç†
                messages.extend(self._handle_values_chunk_selective(chunk))
            
            else:
                # æœªè¯†åˆ«çš„chunkæ ¼å¼ï¼Œå°è¯•é€šç”¨å¤„ç†
                logger.debug(f"[DEBUG] æœªè¯†åˆ«çš„chunkæ ¼å¼ï¼Œå°è¯•é€šç”¨å¤„ç†")
                fallback_messages = self._handle_unknown_chunk(chunk)
                messages.extend(fallback_messages)
                
        except Exception as e:
            logger.error(f"å¤„ç†æµæ•°æ®å—æ—¶å‡ºé”™: {str(e)}, chunkç±»å‹: {type(chunk)}")
            error_msg = create_message(
                StreamMessageType.ERROR,
                session_id=self.session_id,
                source="chunk_processor",
                error_message=f"æ•°æ®å—å¤„ç†é”™è¯¯: {str(e)}"
            )
            messages.append(error_msg)
        
        return messages
    
    def _is_message_chunk(self, chunk: Any) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºæ¶ˆæ¯æµæ•°æ®å— - åŸºäºå®˜æ–¹æ–‡æ¡£ä¼˜åŒ–
        
        LangGraphå®˜æ–¹æ¶ˆæ¯æµæ ¼å¼:
        1. ('messages', (LLM_token, metadata)) - æ ‡å‡†æ ¼å¼
        2. (LLM_token, metadata) - ç›´æ¥æ ¼å¼
        """
        if not isinstance(chunk, tuple) or len(chunk) < 2:
            return False
        
        # æ ¼å¼1: ('messages', ...)
        if chunk[0] == 'messages':
            return True
        
        # æ’é™¤å…¶ä»–å·²çŸ¥çš„æµç±»å‹æ ‡è¯†ç¬¦
        if chunk[0] in ['custom', 'updates', 'values', 'debug']:
            return False
        
        # æ ¼å¼2: (LLM_token, metadata) - æ£€æŸ¥ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯å¦ä¸ºLLMè¾“å‡º
        first_element = chunk[0]
        second_element = chunk[1]
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯LangChainæ¶ˆæ¯å¯¹è±¡
        try:
            from langchain_core.messages import BaseMessage
            if isinstance(first_element, BaseMessage):
                return True
        except ImportError:
            # å›é€€æ£€æŸ¥ï¼šæ˜¯å¦æœ‰æ¶ˆæ¯å¯¹è±¡çš„å…¸å‹å±æ€§
            if hasattr(first_element, 'content'):
                return True
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯å­—ç¬¦ä¸²tokenï¼ˆLLMçš„æ–‡æœ¬è¾“å‡ºï¼‰
        # ä½†è¦ç¡®ä¿ç¬¬ä¸€ä¸ªå…ƒç´ ä¸æ˜¯æµç±»å‹æ ‡è¯†ç¬¦
        if isinstance(first_element, str) and first_element.strip():
            # ç¬¬äºŒä¸ªå…ƒç´ åº”è¯¥æ˜¯metadataå­—å…¸
            if isinstance(second_element, dict):
                # è¿›ä¸€æ­¥æ£€æŸ¥ï¼šç¡®ä¿ä¸æ˜¯å…¶ä»–æµæ ¼å¼çš„è¯¯åˆ¤
                # LLM tokené€šå¸¸ä¸ä¼šæ˜¯å•ä¸ªå¸¸è§è¯æ±‡
                if len(first_element) > 2 and not first_element.isalpha():
                    return True
        
        return False
    
    def _is_update_chunk(self, chunk: Any) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ›´æ–°æµæ•°æ®å—"""
        # LangGraphæ›´æ–°æµæ ¼å¼: ('updates', {...})
        if isinstance(chunk, tuple) and len(chunk) == 2 and chunk[0] == 'updates':
            return True
        return False
    
    def _is_custom_chunk(self, chunk: Any) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºè‡ªå®šä¹‰æµæ•°æ®å—"""
        # LangGraphè‡ªå®šä¹‰æµæ ¼å¼: ('custom', {...})
        if isinstance(chunk, tuple) and len(chunk) == 2 and chunk[0] == 'custom':
            return True
        return False
    
    def _is_values_chunk(self, chunk: Any) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºå€¼æµæ•°æ®å—"""
        # LangGraphå€¼æµæ ¼å¼: ('values', {...})
        if isinstance(chunk, tuple) and len(chunk) == 2 and chunk[0] == 'values':
            return True
        return False

    def _handle_message_chunk(self, chunk: tuple) -> List[BaseStreamMessage]:
        """
        å¤„ç†æ¶ˆæ¯æµæ•°æ®å— - åŸºäºçœŸå®æ ¼å¼ä¼˜åŒ–
        
        çœŸå®æ ¼å¼ç¤ºä¾‹:
        ('messages', (AIMessageChunk(content='```'), metadata))
        """
        messages = []
        
        if len(chunk) < 2:
            logger.warning(f"[DEBUG] æ¶ˆæ¯chunkæ ¼å¼é”™è¯¯: {chunk}")
            return messages
        
        logger.debug(f"[DEBUG] å¤„ç†æ¶ˆæ¯chunk: é•¿åº¦={len(chunk)}")
        
        try:
            from langchain_core.messages import AIMessage, AIMessageChunk, HumanMessage, SystemMessage, ToolMessage, BaseMessage
            
            # è§£æchunkæ ¼å¼
            if chunk[0] == 'messages':
                # æ ‡å‡†æ ¼å¼: ('messages', message_data)
                message_data = chunk[1]
                
                if isinstance(message_data, tuple) and len(message_data) >= 1:
                    # ('messages', (message, metadata))
                    message = message_data[0]
                    metadata = message_data[1] if len(message_data) > 1 else {}
                else:
                    # ('messages', message)
                    message = message_data
                    metadata = {}
            else:
                # ç›´æ¥æ ¼å¼: (LLM token, metadata)
                message = chunk[0]
                metadata = chunk[1] if len(chunk) > 1 else {}
            
            # å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯
            if isinstance(message, (AIMessage, AIMessageChunk)):
                # AIæ¶ˆæ¯ - LLMè¾“å‡º
                content = getattr(message, 'content', '')
                
                if content and str(content).strip():  # åªå¤„ç†éç©ºå†…å®¹
                    # ç¡®å®šæ˜¯å¦ä¸ºå®Œæ•´æ¶ˆæ¯
                    is_complete = isinstance(message, AIMessage)
                    
                    # ä»metadataä¸­æå–èŠ‚ç‚¹ä¿¡æ¯
                    source = "ai_assistant"
                    llm_model = "unknown"
                    if isinstance(metadata, dict):
                        source = metadata.get('langgraph_node', metadata.get('node', source))
                        llm_model = metadata.get('ls_model_name', llm_model)
                    
                    llm_msg = create_message(
                        StreamMessageType.LLM_TOKEN,
                        session_id=self.session_id,
                        source=source,
                        content=str(content),
                        is_complete=is_complete,
                        llm_model=llm_model,
                        metadata=metadata if isinstance(metadata, dict) else {}
                    )
                    messages.append(llm_msg)
                    logger.debug(f"[DEBUG] åˆ›å»ºLLMæ¶ˆæ¯: å®Œæ•´={is_complete}, é•¿åº¦={len(str(content))}")
            
            elif isinstance(message, ToolMessage):
                # å·¥å…·æ¶ˆæ¯
                tool_msg = create_message(
                    StreamMessageType.TOOL_COMPLETE,
                    session_id=self.session_id,
                    source="tool_executor",
                    tool_name=getattr(message, 'name', 'unknown'),
                    action="complete",
                    output=getattr(message, 'content', ''),
                    metadata=metadata if isinstance(metadata, dict) else {}
                )
                messages.append(tool_msg)
                logger.debug(f"[DEBUG] åˆ›å»ºå·¥å…·æ¶ˆæ¯")
            
            # æ³¨æ„ï¼šHumanMessageé€šå¸¸ä¸éœ€è¦æ¨é€åˆ°å‰ç«¯ï¼Œå› ä¸ºæ˜¯ç”¨æˆ·è¾“å…¥
            
        except ImportError:
            # é™çº§å¤„ç†ï¼šæ— æ³•å¯¼å…¥LangChainæ¶ˆæ¯ç±»å‹
            logger.warning("æ— æ³•å¯¼å…¥LangChainæ¶ˆæ¯ç±»å‹ï¼Œä½¿ç”¨é™çº§å¤„ç†")
            message = chunk[1] if chunk[0] == 'messages' else chunk[0]
            content = getattr(message, 'content', str(message))
            
            if content and str(content).strip():
                llm_msg = create_message(
                    StreamMessageType.LLM_TOKEN,
                    session_id=self.session_id,
                    source="ai_assistant",
                    content=str(content),
                    is_complete=True
                )
                messages.append(llm_msg)
        
        logger.debug(f"[DEBUG] æ¶ˆæ¯chunkå¤„ç†ç»“æœ: ç”Ÿæˆäº† {len(messages)} ä¸ªæ¶ˆæ¯")
        return messages

    def _handle_update_chunk(self, chunk: tuple) -> List[BaseStreamMessage]:
        """
        å¤„ç†æ›´æ–°æµæ•°æ®å— - åŸºäºçœŸå®æ ¼å¼ç®€åŒ–ä¼˜åŒ–
        
        çœŸå®æ ¼å¼ç¤ºä¾‹:
        ('updates', {'meta_supervisor': {å®Œæ•´èŠ‚ç‚¹çŠ¶æ€}})
        
        æ ¹æ®å®é™…éœ€æ±‚ï¼Œæˆ‘ä»¬ä¸»è¦å…³å¿ƒ:
        1. èŠ‚ç‚¹æ‰§è¡ŒçŠ¶æ€ï¼ˆå¼€å§‹/å®Œæˆ/é”™è¯¯ï¼‰
        2. è·¯ç”±å†³ç­–ä¿¡æ¯
        3. ä»»åŠ¡æ‰§è¡Œè¿›åº¦
        """
        messages = []
        
        if len(chunk) != 2 or chunk[0] != 'updates':
            logger.warning(f"[DEBUG] æ›´æ–°chunkæ ¼å¼é”™è¯¯: {chunk}")
            return messages
        
        update_data = chunk[1]
        if not isinstance(update_data, dict):
            logger.warning(f"[DEBUG] æ›´æ–°æ•°æ®ä¸æ˜¯å­—å…¸æ ¼å¼: {type(update_data)}")
            return messages
        
        logger.debug(f"[DEBUG] å¤„ç†updates: èŠ‚ç‚¹æ•°é‡={len(update_data)}")
        
        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹çš„æ›´æ–°ç”Ÿæˆæ¶ˆæ¯
        for node_name, node_data in update_data.items():
            logger.debug(f"[DEBUG] å¤„ç†èŠ‚ç‚¹: {node_name}")
            
            # ç”ŸæˆèŠ‚ç‚¹çŠ¶æ€æ›´æ–°æ¶ˆæ¯
            if isinstance(node_data, dict):
                # æ£€æµ‹èŠ‚ç‚¹çŠ¶æ€
                status_info = self._analyze_node_status(node_name, node_data)
                
                # åˆ›å»ºèŠ‚ç‚¹çŠ¶æ€æ¶ˆæ¯
                if status_info['type'] in ['start', 'complete', 'error']:
                    message_type_map = {
                        'start': StreamMessageType.NODE_START,
                        'complete': StreamMessageType.NODE_COMPLETE,
                        'error': StreamMessageType.NODE_ERROR
                    }
                    
                    node_msg = create_message(
                        message_type_map[status_info['type']],
                        session_id=self.session_id,
                        source=node_name,
                        node_name=node_name,
                        status=status_info['status'],
                        details=status_info['details']
                    )
                    messages.append(node_msg)
                
                # æ£€æµ‹è·¯ç”±ä¿¡æ¯
                route_info = self._extract_route_info(node_name, node_data)
                if route_info:
                    route_msg = create_message(
                        StreamMessageType.ROUTE_DECISION,
                        session_id=self.session_id,
                        source=node_name,
                        from_node=node_name,
                        to_node=route_info['to_node'],
                        reason=route_info['reason']
                    )
                    messages.append(route_msg)
            
            else:
                # ç®€å•çŠ¶æ€æè¿°
                info_msg = create_message(
                    StreamMessageType.INFO,
                    session_id=self.session_id,
                    source=node_name,
                    content=f"èŠ‚ç‚¹ {node_name}: {str(node_data)[:100]}"
                )
                messages.append(info_msg)
        
        logger.debug(f"[DEBUG] updateså¤„ç†ç»“æœ: ç”Ÿæˆäº† {len(messages)} ä¸ªæ¶ˆæ¯")
        return messages
    
    def _analyze_node_status(self, node_name: str, node_data: dict) -> dict:
        """åˆ†æèŠ‚ç‚¹çŠ¶æ€"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«çŠ¶æ€æŒ‡ç¤ºä¿¡æ¯
        status_str = str(node_data).lower()
        
        if any(word in status_str for word in ['start', 'begin', 'running', 'executing']):
            if node_name not in self.active_nodes:
                self.active_nodes[node_name] = time.time()
            return {
                'type': 'start',
                'status': 'started',
                'details': f"èŠ‚ç‚¹ {node_name} å¼€å§‹æ‰§è¡Œ"
            }
        elif any(word in status_str for word in ['complete', 'finish', 'done', 'end']):
            start_time = self.active_nodes.pop(node_name, time.time())
            duration = time.time() - start_time
            return {
                'type': 'complete',
                'status': 'completed',
                'details': f"èŠ‚ç‚¹ {node_name} æ‰§è¡Œå®Œæˆï¼Œè€—æ—¶ {duration:.2f}s"
            }
        elif any(word in status_str for word in ['error', 'fail', 'exception']):
            return {
                'type': 'error',
                'status': 'error',
                'details': f"èŠ‚ç‚¹ {node_name} æ‰§è¡Œå‡ºé”™"
            }
        else:
            return {
                'type': 'update',
                'status': 'updated',
                'details': f"èŠ‚ç‚¹ {node_name} çŠ¶æ€æ›´æ–°"
            }
    
    def _extract_route_info(self, node_name: str, node_data: dict) -> Optional[dict]:
        """æå–è·¯ç”±ä¿¡æ¯"""
        # å°è¯•ä»å…ƒæ•°æ®ä¸­æå–è·¯ç”±ä¿¡æ¯
        metadata = node_data.get('metadata', {})
        if isinstance(metadata, dict):
            # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰§è¡Œç­–ç•¥ï¼ˆå¯èƒ½åŒ…å«è·¯ç”±ä¿¡æ¯ï¼‰
            execution_strategy = metadata.get('execution_strategy', {})
            if execution_strategy:
                return {
                    'to_node': 'next_node',  # è¿™é‡Œéœ€è¦æ ¹æ®å®é™…ç­–ç•¥æå–
                    'reason': 'æ‰§è¡Œç­–ç•¥å†³ç­–'
                }
        
        return None

    def _handle_custom_chunk(self, chunk: tuple) -> List[BaseStreamMessage]:
        """
        å¤„ç†è‡ªå®šä¹‰æµæ•°æ®å— - æ‰©å±•æ”¯æŒæ›´å¤šæ¶ˆæ¯ç±»å‹
        
        çœŸå®æ ¼å¼ç¤ºä¾‹:
        ('custom', {'agent_thinking': 'ğŸ” main_agent æ­£åœ¨åˆ†ææ‚¨çš„è¯·æ±‚...'})
        
        åŸºäºget_stream_writerçš„ä½¿ç”¨æ¨¡å¼ï¼Œæ”¯æŒ:
        1. agent_thinking - Agentæ€è€ƒè¿‡ç¨‹
        2. tool_progress - å·¥å…·æ‰§è¡Œè¿›åº¦  
        3. file_generated - æ–‡ä»¶ç”Ÿæˆé€šçŸ¥
        4. node_execution - èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…
        5. route_decision - è·¯ç”±å†³ç­–
        6. task_status - ä»»åŠ¡çŠ¶æ€æ›´æ–°
        7. llm_response - LLMå“åº”å†…å®¹
        8. error_info - é”™è¯¯ä¿¡æ¯
        9. analysis_result - åˆ†æç»“æœ
        """
        messages = []
        
        if len(chunk) != 2 or chunk[0] != 'custom':
            logger.warning(f"[DEBUG] è‡ªå®šä¹‰chunkæ ¼å¼é”™è¯¯: {chunk}")
            return messages
        
        custom_data = chunk[1]
        if not isinstance(custom_data, dict):
            logger.warning(f"[DEBUG] è‡ªå®šä¹‰æ•°æ®ä¸æ˜¯å­—å…¸æ ¼å¼: {type(custom_data)}")
            return messages
        
        logger.debug(f"[DEBUG] å¤„ç†custom: é”®={list(custom_data.keys())}")
        
        # å¤„ç†Agentæ€è€ƒæ¶ˆæ¯
        if "agent_thinking" in custom_data:
            thinking_data = custom_data["agent_thinking"]
            if isinstance(thinking_data, dict):
                # ç»“æ„åŒ–æ€è€ƒæ•°æ®
                agent_name = thinking_data.get("agent_name", "unknown")
                content = thinking_data.get("content", "")
                thinking_type = thinking_data.get("thinking_type", "analysis")
            elif isinstance(thinking_data, str):
                # ç®€å•å­—ç¬¦ä¸²æ ¼å¼
                content = thinking_data
                agent_name = "unknown"
                thinking_type = "analysis"
                
                # å°è¯•ä»å†…å®¹ä¸­æå–agentåç§°
                if "main_agent" in thinking_data:
                    agent_name = "main_agent"
                elif "data_agent" in thinking_data:
                    agent_name = "data_agent"
                elif "expert_agent" in thinking_data:
                    agent_name = "expert_agent"
            else:
                logger.warning(f"[DEBUG] ä¸æ”¯æŒçš„agent_thinkingæ ¼å¼: {type(thinking_data)}")
                content = str(thinking_data)
                agent_name = "unknown"
                thinking_type = "analysis"
            
            thinking_msg = create_message(
                StreamMessageType.AGENT_THINKING,
                session_id=self.session_id,
                source=agent_name,
                agent_name=agent_name,
                thinking_type=thinking_type,
                content=content
            )
            messages.append(thinking_msg)
            logger.debug(f"[DEBUG] åˆ›å»ºAgentæ€è€ƒæ¶ˆæ¯: {agent_name}")
        
        # å¤„ç†å·¥å…·è¿›åº¦æ¶ˆæ¯
        if "tool_progress" in custom_data:
            progress_data = custom_data["tool_progress"]
            if isinstance(progress_data, dict):
                progress_msg = create_message(
                    StreamMessageType.TOOL_PROGRESS,
                    session_id=self.session_id,
                    source=progress_data.get("source", "tool"),
                    tool_name=progress_data.get("tool_name", "unknown"),
                    action="progress",
                    progress=progress_data.get("progress", 0.0),
                    details=progress_data.get("details", "")
                )
                messages.append(progress_msg)
                logger.debug(f"[DEBUG] åˆ›å»ºå·¥å…·è¿›åº¦æ¶ˆæ¯")
        
        # å¤„ç†æ–‡ä»¶ç”Ÿæˆæ¶ˆæ¯
        if "file_generated" in custom_data:
            file_data = custom_data["file_generated"]
            if isinstance(file_data, dict):
                file_msg = create_message(
                    StreamMessageType.FILE_GENERATED,
                    session_id=self.session_id,
                    source=file_data.get("source", "system"),
                    file_id=file_data.get("file_id", ""),
                    file_name=file_data.get("file_name", "unknown"),
                    file_type=file_data.get("file_type", "unknown"),
                    file_path=file_data.get("file_path", ""),
                    category="generated"
                )
                messages.append(file_msg)
                logger.debug(f"[DEBUG] åˆ›å»ºæ–‡ä»¶ç”Ÿæˆæ¶ˆæ¯")
        
        # å¤„ç†èŠ‚ç‚¹æ‰§è¡Œè¯¦æƒ…
        if "node_execution" in custom_data:
            exec_data = custom_data["node_execution"]
            if isinstance(exec_data, dict):
                action = exec_data.get("action", "update")
                if action == "start":
                    message_type = StreamMessageType.NODE_START
                elif action == "complete":
                    message_type = StreamMessageType.NODE_COMPLETE
                elif action == "error":
                    message_type = StreamMessageType.NODE_ERROR
                else:
                    message_type = StreamMessageType.INFO
                
                if message_type == StreamMessageType.INFO:
                    node_msg = create_message(
                        message_type,
                        session_id=self.session_id,
                        source=exec_data.get("node_name", "unknown"),
                        content=f"èŠ‚ç‚¹ {exec_data.get('node_name', 'unknown')} çŠ¶æ€: {exec_data.get('status', 'unknown')}"
                    )
                else:
                    node_msg = create_message(
                        message_type,
                        session_id=self.session_id,
                        source=exec_data.get("node_name", "unknown"),
                        node_name=exec_data.get("node_name", "unknown"),
                        status=exec_data.get("status", "unknown"),
                        details=exec_data.get("details", "")
                    )
                messages.append(node_msg)
                logger.debug(f"[DEBUG] åˆ›å»ºèŠ‚ç‚¹æ‰§è¡Œæ¶ˆæ¯")
        
        # å¤„ç†è·¯ç”±å†³ç­–
        if "route_decision" in custom_data:
            route_data = custom_data["route_decision"]
            if isinstance(route_data, dict):
                route_msg = create_message(
                    StreamMessageType.ROUTE_DECISION,
                    session_id=self.session_id,
                    source=route_data.get("from_node", "router"),
                    from_node=route_data.get("from_node", "unknown"),
                    to_node=route_data.get("to_node", "unknown"),
                    reason=route_data.get("reason", "")
                )
                messages.append(route_msg)
                logger.debug(f"[DEBUG] åˆ›å»ºè·¯ç”±å†³ç­–æ¶ˆæ¯")
        
        # å¤„ç†ä»»åŠ¡çŠ¶æ€æ›´æ–°
        if "task_status" in custom_data:
            task_data = custom_data["task_status"]
            if isinstance(task_data, dict):
                task_name = task_data.get("task_name", "unknown")
                status = task_data.get("status", "unknown")
                progress = task_data.get("progress", 0.0)
                details = task_data.get("details", "")
                
                # æ ¹æ®çŠ¶æ€ç¡®å®šæ¶ˆæ¯ç±»å‹
                if status in ["started", "running"]:
                    message_type = StreamMessageType.TOOL_START
                    action = "start"
                elif status in ["completed", "finished"]:
                    message_type = StreamMessageType.TOOL_COMPLETE
                    action = "complete"
                elif status in ["failed", "error"]:
                    message_type = StreamMessageType.TOOL_ERROR
                    action = "error"
                else:
                    message_type = StreamMessageType.TOOL_PROGRESS
                    action = "progress"
                
                task_msg = create_message(
                    message_type,
                    session_id=self.session_id,
                    source="task_manager",
                    tool_name=task_name,
                    action=action,
                    progress=progress,
                    details=details if details else f"ä»»åŠ¡ {task_name} çŠ¶æ€: {status}"
                )
                messages.append(task_msg)
                logger.debug(f"[DEBUG] åˆ›å»ºä»»åŠ¡çŠ¶æ€æ¶ˆæ¯: {task_name} - {status}")
        
        # å¤„ç†LLMå“åº”å†…å®¹
        if "llm_response" in custom_data:
            llm_data = custom_data["llm_response"]
            if isinstance(llm_data, dict):
                content = llm_data.get("content", "")
                model_name = llm_data.get("model_name", "unknown")
                is_complete = llm_data.get("is_complete", False)
                
                if content and content.strip():
                    llm_msg = create_message(
                        StreamMessageType.LLM_TOKEN,
                        session_id=self.session_id,
                        source="custom_llm",
                        content=content,
                        is_complete=is_complete,
                        llm_model=model_name
                    )
                    messages.append(llm_msg)
                    logger.debug(f"[DEBUG] åˆ›å»ºè‡ªå®šä¹‰LLMå“åº”æ¶ˆæ¯")
        
        # å¤„ç†é”™è¯¯ä¿¡æ¯
        if "error_info" in custom_data:
            error_data = custom_data["error_info"]
            if isinstance(error_data, dict):
                error_msg = create_message(
                    StreamMessageType.ERROR,
                    session_id=self.session_id,
                    source=error_data.get("source", "system"),
                    error_message=error_data.get("error_message", ""),
                    error_code=error_data.get("error_code", "")
                )
                messages.append(error_msg)
                logger.debug(f"[DEBUG] åˆ›å»ºé”™è¯¯ä¿¡æ¯æ¶ˆæ¯")
        
        # å¤„ç†åˆ†æç»“æœ
        if "analysis_result" in custom_data:
            analysis_data = custom_data["analysis_result"]
            if isinstance(analysis_data, dict):
                result_type = analysis_data.get("result_type", "unknown")
                result_data = analysis_data.get("result_data", {})
                confidence = analysis_data.get("confidence", 0.0)
                
                # åˆ›å»ºåˆ†æç»“æœä¿¡æ¯æ¶ˆæ¯
                content = f"åˆ†æç»“æœ ({result_type}): ç½®ä¿¡åº¦ {confidence:.2%}"
                if isinstance(result_data, dict) and result_data:
                    # æ·»åŠ å…³é”®ç»“æœä¿¡æ¯
                    key_info = []
                    for key, value in list(result_data.items())[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé”®å€¼å¯¹
                        key_info.append(f"{key}: {value}")
                    content += f" | {', '.join(key_info)}"
                
                analysis_msg = create_message(
                    StreamMessageType.INFO,
                    session_id=self.session_id,
                    source="analysis_engine",
                    content=content,
                    metadata={
                        "result_type": result_type,
                        "confidence": confidence,
                        "result_data": result_data
                    }
                )
                messages.append(analysis_msg)
                logger.debug(f"[DEBUG] åˆ›å»ºåˆ†æç»“æœæ¶ˆæ¯: {result_type}")
        
        # å¤„ç†é€šç”¨è‡ªå®šä¹‰æ¶ˆæ¯
        handled_keys = {
            "agent_thinking", "tool_progress", "file_generated", "node_execution", 
            "route_decision", "task_status", "llm_response", "error_info", "analysis_result"
        }
        
        for key, value in custom_data.items():
            if key not in handled_keys:
                info_msg = create_message(
                    StreamMessageType.INFO,
                    session_id=self.session_id,
                    source="custom",
                    content=f"{key}: {str(value)[:200]}{'...' if len(str(value)) > 200 else ''}"
                )
                messages.append(info_msg)
                logger.debug(f"[DEBUG] åˆ›å»ºé€šç”¨è‡ªå®šä¹‰æ¶ˆæ¯: {key}")
        
        logger.debug(f"[DEBUG] customå¤„ç†ç»“æœ: ç”Ÿæˆäº† {len(messages)} ä¸ªæ¶ˆæ¯")
        return messages

    def _handle_values_chunk_selective(self, chunk: tuple) -> List[BaseStreamMessage]:
        """
        é€‰æ‹©æ€§å¤„ç†å€¼æµæ•°æ®å—
        
        valuesæµåŒ…å«å®Œæ•´çŠ¶æ€å¿«ç…§ï¼Œæ•°æ®é‡å¤§ï¼Œé€šå¸¸ä¸æ¨é€åˆ°å‰ç«¯
        åªåœ¨éœ€è¦ç‰¹å®šä¿¡æ¯æ—¶æ‰æå–å¿…è¦éƒ¨åˆ†
        """
        messages = []
        
        if len(chunk) != 2 or chunk[0] != 'values':
            return messages
        
        values_data = chunk[1]
        if not isinstance(values_data, dict):
            return messages
        
        logger.debug(f"[DEBUG] values chunkåŒ…å«: {list(values_data.keys())}")
        
        # åªæå–æœ€æ–°çš„AIå›å¤ï¼ˆå¦‚æœæœ‰ï¼‰
        messages_list = values_data.get('messages', [])
        if messages_list:
            try:
                # è·å–æœ€åä¸€æ¡AIæ¶ˆæ¯
                for msg in reversed(messages_list):
                    if hasattr(msg, 'content') and getattr(msg, '__class__', None).__name__ in ['AIMessage']:
                        content = getattr(msg, 'content', '')
                        if content and 'å®¡æŸ¥é€šè¿‡' not in content and 'ä»»åŠ¡åˆ†æå®Œæˆ' not in content:
                            # è¿™æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„AIå›å¤
                            ai_msg = create_message(
                                StreamMessageType.INFO,
                                session_id=self.session_id,
                                source="ai_assistant",
                                content=f"AIå›å¤: {str(content)[:100]}{'...' if len(str(content)) > 100 else ''}"
                            )
                            messages.append(ai_msg)
                            logger.debug(f"[DEBUG] ä»valuesä¸­æå–AIå›å¤")
                            break
            except Exception as e:
                logger.debug(f"[DEBUG] æå–valuesä¸­AIæ¶ˆæ¯å¤±è´¥: {e}")
        
        # æå–ä»»åŠ¡çŠ¶æ€ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        current_task = values_data.get('current_task')
        if current_task:
            task_msg = create_message(
                StreamMessageType.INFO,
                session_id=self.session_id,
                source="task_manager",
                content=f"å½“å‰ä»»åŠ¡: {str(current_task)[:100]}"
            )
            messages.append(task_msg)
            logger.debug(f"[DEBUG] ä»valuesä¸­æå–ä»»åŠ¡ä¿¡æ¯")
        
        logger.debug(f"[DEBUG] valuesé€‰æ‹©æ€§å¤„ç†ç»“æœ: ç”Ÿæˆäº† {len(messages)} ä¸ªæ¶ˆæ¯")
        return messages

    def _handle_unknown_chunk(self, chunk: Any) -> List[BaseStreamMessage]:
        """å¤„ç†æœªè¯†åˆ«çš„æ•°æ®å—"""
        messages = []
        
        # å°è¯•é€šç”¨å¤„ç†
        try:
            if isinstance(chunk, dict):
                # å­—å…¸æ ¼å¼ï¼Œå°è¯•æå–æœ‰ç”¨ä¿¡æ¯
                for key, value in chunk.items():
                    if key.startswith('__') and key.endswith('__'):
                        continue  # è·³è¿‡ç³»ç»Ÿå±æ€§
                    
                    info_msg = create_message(
                        StreamMessageType.INFO,
                        session_id=self.session_id,
                        source="unknown",
                        content=f"{key}: {str(value)[:200]}"
                    )
                    messages.append(info_msg)
            
            elif isinstance(chunk, (str, int, float)):
                # ç®€å•ç±»å‹
                if str(chunk).strip():
                    info_msg = create_message(
                        StreamMessageType.INFO,
                        session_id=self.session_id,
                        source="unknown",
                        content=str(chunk)[:200]
                    )
                    messages.append(info_msg)
            
            else:
                # å…¶ä»–ç±»å‹ï¼Œè®°å½•ä½†ä¸å¤„ç†
                logger.debug(f"[DEBUG] è·³è¿‡æœªè¯†åˆ«chunkç±»å‹: {type(chunk)}")
                
        except Exception as e:
            logger.error(f"é€šç”¨å¤„ç†chunkå¤±è´¥: {e}")
        
        return messages

    def _handle_node_start(self, event_data: Dict[str, Any]) -> BaseStreamMessage:
        """å¤„ç†èŠ‚ç‚¹å¼€å§‹äº‹ä»¶"""
        node_name = event_data.get("name", "unknown")
        self.active_nodes[node_name] = time.time()
        
        return create_message(
            StreamMessageType.NODE_START,
            session_id=self.session_id,
            source=node_name,
            node_name=node_name,
            status="started"
        )
    
    def _handle_node_end(self, event_data: Dict[str, Any]) -> BaseStreamMessage:
        """å¤„ç†èŠ‚ç‚¹ç»“æŸäº‹ä»¶"""
        node_name = event_data.get("name", "unknown")
        start_time = self.active_nodes.pop(node_name, time.time())
        duration = time.time() - start_time
        
        return create_message(
            StreamMessageType.NODE_COMPLETE,
            session_id=self.session_id,
            source=node_name,
            node_name=node_name,
            status="completed",
            duration=duration
        )
    
    def _handle_node_error(self, event_data: Dict[str, Any]) -> BaseStreamMessage:
        """å¤„ç†èŠ‚ç‚¹é”™è¯¯äº‹ä»¶"""
        node_name = event_data.get("name", "unknown")
        error_msg = event_data.get("error", "Unknown error")
        
        return create_message(
            StreamMessageType.NODE_ERROR,
            session_id=self.session_id,
            source=node_name,
            node_name=node_name,
            status="error",
            details=str(error_msg)
        )
    
    def _handle_llm_start(self, event_data: Dict[str, Any]) -> BaseStreamMessage:
        """å¤„ç†LLMå¼€å§‹äº‹ä»¶"""
        return create_message(
            StreamMessageType.INFO,
            session_id=self.session_id,
            source="llm",
            content="å¼€å§‹ç”Ÿæˆå›ç­”...",
            level="info"
        )
    
    def _handle_llm_token(self, event_data: Dict[str, Any]) -> BaseStreamMessage:
        """å¤„ç†LLM Tokenäº‹ä»¶"""
        token = event_data.get("token", "")
        
        return create_message(
            StreamMessageType.LLM_TOKEN,
            session_id=self.session_id,
            source="llm",
            content=token,
            is_complete=False
        )
    
    def _handle_llm_end(self, event_data: Dict[str, Any]) -> BaseStreamMessage:
        """å¤„ç†LLMç»“æŸäº‹ä»¶"""
        return create_message(
            StreamMessageType.LLM_COMPLETE,
            session_id=self.session_id,
            source="llm",
            content="",
            is_complete=True
        )
    
    def _handle_tool_start(self, event_data: Dict[str, Any]) -> BaseStreamMessage:
        """å¤„ç†å·¥å…·å¼€å§‹äº‹ä»¶"""
        tool_name = event_data.get("name", "unknown")
        
        return create_message(
            StreamMessageType.TOOL_START,
            session_id=self.session_id,
            source=tool_name,
            tool_name=tool_name,
            action="start"
        )
    
    def _handle_tool_end(self, event_data: Dict[str, Any]) -> BaseStreamMessage:
        """å¤„ç†å·¥å…·ç»“æŸäº‹ä»¶"""
        tool_name = event_data.get("name", "unknown")
        output = event_data.get("output", "")
        
        return create_message(
            StreamMessageType.TOOL_COMPLETE,
            session_id=self.session_id,
            source=tool_name,
            tool_name=tool_name,
            action="complete",
            output=output
        )
    
    def _handle_tool_error(self, event_data: Dict[str, Any]) -> BaseStreamMessage:
        """å¤„ç†å·¥å…·é”™è¯¯äº‹ä»¶"""
        tool_name = event_data.get("name", "unknown")
        error = event_data.get("error", "Unknown error")
        
        return create_message(
            StreamMessageType.TOOL_ERROR,
            session_id=self.session_id,
            source=tool_name,
            tool_name=tool_name,
            action="error",
            error_message=str(error)
        )


# æ³¨æ„ï¼šæµå¼æ¶ˆæ¯æ¨é€åŠŸèƒ½å·²ç§»è‡³ app/core/stream_writer_helper.py
# è¯·ä½¿ç”¨ stream_writer_helper.py ä¸­çš„æ¨é€å‡½æ•°æ¥é¿å…é‡å¤åŠŸèƒ½
# 
# ä½¿ç”¨ç¤ºä¾‹ï¼š
# from app.core.stream_writer_helper import push_thinking, push_file, push_progress
# 
# push_thinking("agent_name", "æ€è€ƒå†…å®¹")
# push_file("file_id", "file_name", "file_path", "file_type")
# push_progress("tool_name", 0.5, "è¿›åº¦è¯¦æƒ…") 