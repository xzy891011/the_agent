"""
LangGraphæµå¼å¤„ç†å™¨ä¸»å…¥å£æ¨¡å—

æä¾›ç»Ÿä¸€çš„æµå¼å¤„ç†æ¥å£ï¼Œå…¼å®¹ç°æœ‰ç³»ç»Ÿ
"""

import logging
import time
from typing import Dict, Any, Generator, List, Optional, Union
from enum import Enum

# å¯¼å…¥æµå¼å¤„ç†å™¨å’Œç±»å‹
from app.ui.streaming_processor import LangGraphStreamingProcessor
from app.ui.streaming_types import (
    StreamMessageType,
    StreamMessage,
    NodeStatusMessage,
    RouterMessage,
    LLMTokenMessage,
    LLMCompleteMessage,
    ToolExecutionMessage,
    FileGeneratedMessage,
    AgentThinkingMessage,
    SystemMessage
)

logger = logging.getLogger(__name__)

class StreamMode(str, Enum):
    """æµæ¨¡å¼æšä¸¾ - å…¼å®¹ç°æœ‰ä»£ç """
    MESSAGES = "messages"
    CUSTOM = "custom"
    UPDATES = "updates"
    VALUES = "values"
    EVENTS = "events"
    DEBUG = "debug"

# ä¸ºäº†å…¼å®¹æ€§ï¼Œæä¾›å¤šç§æµæ¨¡å¼ç»„åˆ
DEFAULT_STREAM_MODES = ["messages", "custom", "updates", "values"]
ALL_STREAM_MODES = ["messages", "custom", "updates", "values", "events", "debug"]

class LangGraphStreamer:
    """
    LangGraphæµå¤„ç†å™¨ - å…¼å®¹ç°æœ‰ä»£ç æ¥å£
    
    è¿™æ˜¯ä¸€ä¸ªé€‚é…å™¨ç±»ï¼Œå†…éƒ¨ä½¿ç”¨æ–°çš„LangGraphStreamingProcessor
    """
    
    def __init__(
        self,
        stream_modes: Optional[List[str]] = None,
        session_id: Optional[str] = None,
        config: Optional[Dict[str, Any]] = None
    ):
        """
        åˆå§‹åŒ–æµå¤„ç†å™¨
        
        Args:
            stream_modes: æµæ¨¡å¼åˆ—è¡¨
            session_id: ä¼šè¯ID
            config: é…ç½®å‚æ•°
        """
        self.stream_modes = stream_modes or DEFAULT_STREAM_MODES
        self.session_id = session_id
        self.config = config or {}
        
        # å†…éƒ¨ä½¿ç”¨æ–°çš„æµå¤„ç†å™¨
        self.processor = LangGraphStreamingProcessor(session_id=session_id)
        
        logger.info(f"LangGraphæµå¤„ç†å™¨åˆå§‹åŒ–ï¼Œä½¿ç”¨æµæ¨¡å¼: {self.stream_modes}")
    
    def process_stream(
        self, 
        stream_generator: Generator[Dict[str, Any], None, None]
    ) -> Generator[Dict[str, Any], None, None]:
        """
        å¤„ç†LangGraphæµè¾“å‡º - å…¼å®¹åŸæœ‰æ¥å£
        
        Args:
            stream_generator: LangGraphçš„æµç”Ÿæˆå™¨
            
        Yields:
            Dict[str, Any]: å¤„ç†åçš„æµæ•°æ®
        """
        logger.info("å¼€å§‹å¤„ç†LangGraphæµè¾“å‡º")
        logger.info("å·²é‡ç½®token_bufferï¼Œç¡®ä¿ä¸ä¼šç´¯ç§¯ä¸Šä¸€è½®å¯¹è¯å†…å®¹")
        
        chunk_count = 0
        try:
            for chunk in stream_generator:
                chunk_count += 1
                logger.debug(f"[DEBUG] åŸå§‹chunkæ¶ˆæ¯å†…å®¹: {chunk}")
                # ä½¿ç”¨æ–°çš„å¤„ç†å™¨å¤„ç†æ•°æ®å—
                processed_chunks = self.processor._process_stream_chunk(chunk)
                
                # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰å¤„ç†ç»“æœï¼Œå°è¯•è½¬æ¢åŸå§‹chunkæ ¼å¼
                if not processed_chunks:
                    # å¤„ç†åŸå§‹chunkï¼Œç¡®ä¿æœ‰roleå’Œcontentå­—æ®µ
                    converted_chunk = self._handle_raw_chunk(chunk)
                    if converted_chunk:
                        yield converted_chunk
                else:
                    # è¿”å›å¤„ç†åçš„æ¶ˆæ¯
                    for processed_chunk in processed_chunks:
                        yield self._convert_to_legacy_format(processed_chunk)
                        
        except Exception as e:
            logger.error(f"æµå¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise
        finally:
            logger.info(f"æµå¤„ç†å®Œæˆï¼Œå…±å¤„ç† {chunk_count} ä¸ªæ•°æ®å—")
    
    def _convert_to_legacy_format(self, stream_message: StreamMessage) -> Dict[str, Any]:
        """
        å°†æ–°æ ¼å¼çš„æµæ¶ˆæ¯è½¬æ¢ä¸ºæ—§æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
        
        Args:
            stream_message: æ–°æ ¼å¼çš„æµæ¶ˆæ¯
            
        Returns:
            Dict[str, Any]: æ—§æ ¼å¼çš„æ•°æ®ï¼ŒåŒ…å«roleå’Œcontentå­—æ®µ
        """
        # åŸºæœ¬æ ¼å¼è½¬æ¢
        result = {
            "type": stream_message.type.value,
            "timestamp": stream_message.timestamp.isoformat(),
            "session_id": stream_message.session_id,
            "source": getattr(stream_message, "source", None) or "unknown"
        }
        
        # æ·»åŠ å…ƒæ•°æ®
        metadata = getattr(stream_message, "metadata", None)
        if metadata:
            result["metadata"] = metadata
        
        # æ ¹æ®æ¶ˆæ¯ç±»å‹æ·»åŠ roleå’Œcontentå­—æ®µä»¥åŠç‰¹å®šå­—æ®µ
        if stream_message.type == StreamMessageType.LLM_TOKEN:
            # LLMæ¶ˆæ¯ä½œä¸ºassistantè§’è‰²
            result["role"] = "assistant"
            result["content"] = getattr(stream_message, "content", "")
            result["is_token"] = True
            result["is_complete"] = getattr(stream_message, "is_complete", False)
            result["token"] = getattr(stream_message, "content", "")  # tokenå°±æ˜¯content
            result["llm_model"] = getattr(stream_message, "llm_model", "")
            
        elif stream_message.type == StreamMessageType.LLM_COMPLETE:
            # LLMå®Œæˆæ¶ˆæ¯ä½œä¸ºassistantè§’è‰²
            result["role"] = "assistant"
            result["content"] = getattr(stream_message, "content", "")
            result["is_token"] = False
            result["is_complete"] = True
            result["llm_model"] = getattr(stream_message, "llm_model", "")
            
        elif stream_message.type in [StreamMessageType.TOOL_START, StreamMessageType.TOOL_COMPLETE, StreamMessageType.TOOL_PROGRESS, StreamMessageType.TOOL_ERROR]:
            # å·¥å…·æ¶ˆæ¯ä½œä¸ºtoolè§’è‰²
            result["role"] = "tool"
            result["content"] = getattr(stream_message, "output", "") or getattr(stream_message, "error_message", "")
            result["tool_name"] = getattr(stream_message, "tool_name", "")
            result["status"] = getattr(stream_message, "action", "")  # actionå­—æ®µæ˜ å°„ä¸ºstatus
            result["action"] = getattr(stream_message, "action", "")
            execution_time = getattr(stream_message, "execution_time", None)
            if execution_time is not None:
                result["execution_time"] = execution_time
                
        elif stream_message.type == StreamMessageType.FILE_GENERATED:
            # æ–‡ä»¶ç”Ÿæˆæ¶ˆæ¯ä½œä¸ºsystemè§’è‰²
            result["role"] = "system"
            file_name = getattr(stream_message, "file_name", "unknown")
            result["content"] = f"å·²ç”Ÿæˆæ–‡ä»¶: {file_name}"
            result["file_id"] = getattr(stream_message, "file_id", "")
            result["file_name"] = file_name
            result["file_path"] = getattr(stream_message, "file_path", "")
            result["file_type"] = getattr(stream_message, "file_type", "")
            result["category"] = getattr(stream_message, "category", "generated")
            
        elif stream_message.type == StreamMessageType.AGENT_THINKING:
            # Agentæ€è€ƒæ¶ˆæ¯ä½œä¸ºsystemè§’è‰²
            result["role"] = "system"
            agent_name = getattr(stream_message, "agent_name", "Agent")
            content = getattr(stream_message, "content", "")
            result["content"] = f"ğŸ¤” {agent_name} æ­£åœ¨æ€è€ƒ: {content}"
            result["agent_name"] = agent_name
            result["thinking_type"] = getattr(stream_message, "thinking_type", "")
            
        elif stream_message.type in [StreamMessageType.NODE_START, StreamMessageType.NODE_COMPLETE, StreamMessageType.NODE_ERROR]:
            # èŠ‚ç‚¹çŠ¶æ€æ¶ˆæ¯ä½œä¸ºsystemè§’è‰²
            result["role"] = "system"
            node_name = getattr(stream_message, "node_name", "unknown")
            status = getattr(stream_message, "status", "")
            result["content"] = f"èŠ‚ç‚¹ {node_name} çŠ¶æ€: {status}"
            result["node_name"] = node_name
            result["status"] = status
            result["details"] = getattr(stream_message, "details", "")
            
        elif stream_message.type in [StreamMessageType.ROUTE_DECISION, StreamMessageType.ROUTE_CHANGE]:
            # è·¯ç”±æ¶ˆæ¯ä½œä¸ºsystemè§’è‰²
            result["role"] = "system"
            to_node = getattr(stream_message, "to_node", "unknown")
            reason = getattr(stream_message, "reason", "")
            result["content"] = f"è·¯ç”±å†³ç­–: è½¬å‘ {to_node} - {reason}"
            result["to_node"] = to_node
            result["reason"] = reason
            
        elif stream_message.type in [StreamMessageType.ERROR, StreamMessageType.INFO]:
            # é”™è¯¯å’Œä¿¡æ¯æ¶ˆæ¯ä½œä¸ºsystemè§’è‰²
            result["role"] = "system"
            result["content"] = getattr(stream_message, "content", "")
            result["level"] = getattr(stream_message, "level", "info")
            if stream_message.type == StreamMessageType.ERROR:
                result["error_message"] = getattr(stream_message, "error_message", "")
                
        else:
            # é»˜è®¤å¤„ç†
            result["role"] = "system"
            result["content"] = str(getattr(stream_message, "content", "") or str(stream_message))
        
        # æ„å»ºdataå­—æ®µç”¨äºå‘åå…¼å®¹æ€§
        result["data"] = {
            key: value for key, value in result.items() 
            if key not in ["type", "timestamp", "session_id", "role", "content"]
        }
        
        return result
    
    def _handle_raw_chunk(self, chunk: Any) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†åŸå§‹chunkï¼Œè½¬æ¢ä¸ºåŒ…å«roleå’Œcontentçš„æ ¼å¼
        
        Args:
            chunk: åŸå§‹chunkæ•°æ®
            
        Returns:
            Dict[str, Any]: åŒ…å«roleå’Œcontentå­—æ®µçš„æ•°æ®ï¼Œæˆ–None
        """
        try:
            # å¤„ç†tupleæ ¼å¼çš„æ¶ˆæ¯chunkï¼ˆLangGraphæ¶ˆæ¯æµæ ¼å¼ï¼‰
            if isinstance(chunk, tuple) and len(chunk) == 2:
                node_name, message = chunk
                
                from langchain_core.messages import AIMessage, AIMessageChunk, ToolMessage, SystemMessage, HumanMessage
                
                if isinstance(message, (AIMessage, AIMessageChunk)):
                    # å¯¹äºAIMessageChunkï¼Œæ£€æŸ¥finish_reasonæ¥åˆ¤æ–­æ˜¯å¦æ˜¯å®Œæ•´æ¶ˆæ¯
                    is_complete = False
                    if hasattr(message, 'response_metadata') and message.response_metadata:
                        is_complete = message.response_metadata.get('finish_reason') is not None
                    
                    return {
                        "role": "assistant",
                        "content": message.content or "",
                        "source": node_name,
                        "type": "ai_message_chunk" if isinstance(message, AIMessageChunk) else "ai_message",
                        "is_token": isinstance(message, AIMessageChunk),
                        "is_complete": is_complete,
                        "timestamp": time.time()
                    }
                elif isinstance(message, ToolMessage):
                    return {
                        "role": "tool",
                        "content": message.content or "",
                        "source": node_name,
                        "type": "tool_message",
                        "tool_name": getattr(message, 'name', 'unknown'),
                        "timestamp": time.time()
                    }
                elif isinstance(message, SystemMessage):
                    return {
                        "role": "system",
                        "content": message.content or "",
                        "source": node_name,
                        "type": "system_message",
                        "timestamp": time.time()
                    }
                elif isinstance(message, HumanMessage):
                    return {
                        "role": "user",
                        "content": message.content or "",
                        "source": node_name,
                        "type": "human_message",
                        "timestamp": time.time()
                    }
            
            # å¤„ç†å·²ç»æ˜¯å­—å…¸æ ¼å¼çš„chunk
            elif isinstance(chunk, dict):
                # å¦‚æœå·²ç»æœ‰roleå­—æ®µï¼Œç›´æ¥è¿”å›
                if "role" in chunk:
                    return chunk
                
                # å¦åˆ™å°è¯•ä»ç±»å‹æ¨æ–­role
                chunk_type = chunk.get("type", "")
                content = chunk.get("content", "")
                
                if "ai" in chunk_type.lower() or "assistant" in chunk_type.lower():
                    role = "assistant"
                elif "tool" in chunk_type.lower():
                    role = "tool"
                elif "system" in chunk_type.lower():
                    role = "system"
                elif "human" in chunk_type.lower() or "user" in chunk_type.lower():
                    role = "user"
                else:
                    role = "system"  # é»˜è®¤ä¸ºsystem
                
                result = chunk.copy()
                result["role"] = role
                if "content" not in result:
                    result["content"] = str(content)
                if "timestamp" not in result:
                    result["timestamp"] = time.time()
                
                return result
            
            # å¤„ç†å…¶ä»–æ ¼å¼
            else:
                return {
                    "role": "system",
                    "content": str(chunk),
                    "type": "raw_data",
                    "timestamp": time.time()
                }
                
        except Exception as e:
            logger.error(f"å¤„ç†åŸå§‹chunkæ—¶å‡ºé”™: {str(e)}")
            return {
                "role": "system",
                "content": f"å¤„ç†æ•°æ®æ—¶å‡ºé”™: {str(e)}",
                "type": "error",
                "timestamp": time.time()
            }
    
    def reset(self):
        """é‡ç½®æµå¤„ç†å™¨çŠ¶æ€"""
        self.processor.reset()
        logger.info("æµå¤„ç†å™¨çŠ¶æ€å·²é‡ç½®")
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æµå¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        # è¿”å›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
        return {
            "session_id": self.session_id,
            "stream_modes": self.stream_modes,
            "message_buffer_size": len(getattr(self.processor, 'message_buffer', [])),
            "active_nodes": len(getattr(self.processor, 'active_nodes', {}))
        }


def get_stream_writer(session_id: Optional[str] = None) -> LangGraphStreamingProcessor:
    """
    è·å–æµå¤„ç†å™¨å®ä¾‹ - å…¼å®¹ç°æœ‰ä»£ç 
    
    âš ï¸ æ³¨æ„ï¼šæ­¤å‡½æ•°è¿”å›çš„æ˜¯æµæ•°æ®å¤„ç†å™¨ï¼Œä¸æ˜¯æ¨é€å™¨ã€‚
    å¦‚éœ€æ¨é€æµå¼æ¶ˆæ¯ï¼Œè¯·ä½¿ç”¨ app.core.stream_writer_helper ä¸­çš„æ¨é€å‡½æ•°ã€‚
    
    Args:
        session_id: ä¼šè¯ID
        
    Returns:
        LangGraphStreamingProcessor: æµå¤„ç†å™¨å®ä¾‹ï¼ˆä»…ç”¨äºå¤„ç†æµæ•°æ®ï¼‰
    """
    return LangGraphStreamingProcessor(session_id=session_id)


def create_stream_processor(
    stream_modes: Optional[List[str]] = None,
    session_id: Optional[str] = None,
    config: Optional[Dict[str, Any]] = None
) -> LangGraphStreamer:
    """
    åˆ›å»ºæµå¤„ç†å™¨çš„å·¥å‚å‡½æ•°
    
    Args:
        stream_modes: æµæ¨¡å¼åˆ—è¡¨
        session_id: ä¼šè¯ID
        config: é…ç½®å‚æ•°
        
    Returns:
        LangGraphStreamer: æµå¤„ç†å™¨å®ä¾‹
    """
    return LangGraphStreamer(
        stream_modes=stream_modes,
        session_id=session_id,
        config=config
    )


# å¯¼å‡ºä¸»è¦ç±»å’Œå‡½æ•°ä¾›å¤–éƒ¨ä½¿ç”¨
__all__ = [
    "LangGraphStreamer",
    "StreamMode",
    "get_stream_writer",
    "create_stream_processor",
    "DEFAULT_STREAM_MODES",
    "ALL_STREAM_MODES",
    # ä»streaming_typesæ¨¡å—å¯¼å‡º
    "StreamMessageType",
    "StreamMessage",
    "NodeStatusMessage",
    "RouterMessage", 
    "LLMTokenMessage",
    "LLMCompleteMessage",
    "ToolExecutionMessage",
    "FileGeneratedMessage",
    "AgentThinkingMessage",
    "SystemMessage",
    # ä»streaming_processoræ¨¡å—å¯¼å‡º
    "LangGraphStreamingProcessor"
] 