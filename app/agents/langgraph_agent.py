"""
LangGraph-based Agent Implementation
åŸºäºLangGraphçš„æ™ºèƒ½ä½“å®ç°ï¼Œå®Œå…¨ä½¿ç”¨æ–°æ¶æ„
"""

import json
import logging
import traceback
from typing import Dict, Any, Optional, List, Callable
from datetime import datetime
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode

from app.core.state import IsotopeSystemState, StateManager
from app.core.task_decorator import task_registry
from app.core.system_capability_registry import system_capability_registry, CapabilityType
from app.tools.registry import get_tool_registry
import time

# å¯¼å…¥è®°å¿†ç³»ç»Ÿæ¨¡å—
from app.core.memory.enhanced_memory_integration import EnhancedMemoryIntegration
from app.core.memory.agent_memory_injector import AgentMemoryInjector
from app.core.memory.agent_memory_filter import AgentMemoryFilter
from app.core.stream_writer_helper import push_thinking, push_error,push_progress
logger = logging.getLogger(__name__)


from app.agents.registry import AgentProtocol

class LangGraphAgent(AgentProtocol):
    """åŸºäºLangGraphçš„æ™ºèƒ½ä½“å®ç°"""
    
    def __init__(
        self,
        name: str,
        role: str,
        llm: Any,
        capabilities: Optional[List[str]] = None,
        config: Optional[Dict[str, Any]] = None,
        memory_integration: Optional[EnhancedMemoryIntegration] = None,
        info_hub: Optional[Any] = None,
        interrupt_manager: Optional[Any] = None,
        message_router: Optional[Any] = None
    ):
        """
        åˆå§‹åŒ–LangGraphæ™ºèƒ½ä½“
        
        Args:
            name: æ™ºèƒ½ä½“åç§°
            role: æ™ºèƒ½ä½“è§’è‰² (data_processing, expert_analysis, visualizationç­‰)
            llm: è¯­è¨€æ¨¡å‹å®ä¾‹
            capabilities: æ™ºèƒ½ä½“èƒ½åŠ›åˆ—è¡¨
            config: é…ç½®å‚æ•°
        """
        self.name = name
        self.role = role
        self.llm = llm
        self.capabilities = capabilities or []
        self.config = config or {}
        
        # å¢å¼ºåŠŸèƒ½æ¨¡å—
        self.memory_integration = memory_integration
        self.info_hub = info_hub
        self.interrupt_manager = interrupt_manager
        self.message_router = message_router
        
        # è®°å¿†æ³¨å…¥å™¨
        self.memory_injector = None
        if self.memory_integration:
            try:
                from app.core.memory.agent_memory_injector import create_agent_memory_injector
                self.memory_injector = create_agent_memory_injector(self.memory_integration)
                logger.info(f"æ™ºèƒ½ä½“ {self.name} è®°å¿†æ³¨å…¥å™¨åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.warning(f"æ™ºèƒ½ä½“ {self.name} è®°å¿†æ³¨å…¥å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        
        # è·å–è¯¥æ™ºèƒ½ä½“å¯ç”¨çš„ä»»åŠ¡
        self._available_tools = self._get_available_tools()
        
        # åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿç»„ä»¶
        self._init_memory_system()
        
        # æ„å»ºæ™ºèƒ½ä½“çš„å­å›¾
        self.graph = self._build_graph()
        
        logger.info(f"åˆå§‹åŒ–LangGraphAgent: {name}, è§’è‰²: {role}, å¯ç”¨å·¥å…·æ•°: {len(self._available_tools)}")
    
    def __call__(self, state: IsotopeSystemState) -> IsotopeSystemState:
        """ä½¿æ™ºèƒ½ä½“å¯¹è±¡å¯è°ƒç”¨ï¼Œå§”æ‰˜ç»™runæ–¹æ³•"""
        return self.run(state)
    
    def get_name(self) -> str:
        """è·å–æ™ºèƒ½ä½“åç§°"""
        return self.name
    
    def get_description(self) -> str:
        """è·å–æ™ºèƒ½ä½“æè¿°"""
        return f"{self.name} - è§’è‰²: {self.role}"
    
    def _init_memory_system(self):
        """åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"""
        try:
            # ç¡®ä¿configä¸ä¸ºNone
            if self.config is None:
                self.config = {}
            
            # åˆå§‹åŒ–è®°å¿†é›†æˆ
            self.memory_integration = EnhancedMemoryIntegration(self.config)
            
            # åˆå§‹åŒ–è®°å¿†æ³¨å…¥å™¨
            self.memory_injector = AgentMemoryInjector(self.memory_integration)
            
            # åˆå§‹åŒ–è®°å¿†ç­›é€‰å™¨
            self.memory_filter = AgentMemoryFilter()
            
            # è®°å¿†é…ç½®
            self.memory_config = self.config.get('memory', {})
            self.use_memory = self.memory_config.get('enabled', True)
            
            if self.use_memory:
                logger.info(f"æ™ºèƒ½ä½“ {self.name} è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–æˆåŠŸ")
            else:
                logger.info(f"æ™ºèƒ½ä½“ {self.name} è®°å¿†ç³»ç»Ÿå·²ç¦ç”¨")
                
        except Exception as e:
            logger.warning(f"æ™ºèƒ½ä½“ {self.name} è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
            self.use_memory = False
            self.memory_integration = None
            self.memory_injector = None
            self.memory_filter = None
    
    def _get_available_tools(self) -> Dict[str, Callable]:
        """è·å–è¯¥æ™ºèƒ½ä½“å¯ç”¨çš„å·¥å…·ï¼ˆåŸºäºè§’è‰²å’Œèƒ½åŠ›ï¼‰"""
        available_tools = {}
        
        # ============ MCPå·¥å…·é›†æˆ ============
        # ä¼˜å…ˆä½¿ç”¨MCPå·¥å…·ï¼ˆå¦‚æœå·²å¯ç”¨ï¼‰
        if self._is_mcp_enabled():
            logger.info(f"æ™ºèƒ½ä½“ {self.name} å¯ç”¨MCPå·¥å…·æ”¯æŒ")
            
            # è·å–æ··åˆå·¥å…·åˆ—è¡¨ï¼ˆä¼ ç»Ÿä»»åŠ¡ + MCPå·¥å…·ï¼‰
            all_tools = self._get_mixed_tools()
            
            # å°†å·¥å…·è½¬æ¢ä¸ºä»»åŠ¡æ¥å£
            for tool in all_tools:
                # åˆ›å»ºå·¥å…·åŒ…è£…å‡½æ•°
                def create_tool_wrapper(tool_instance):
                    def tool_task(**kwargs):
                        """MCPå·¥å…·åŒ…è£…å™¨"""
                        try:
                            # ä½¿ç”¨ç»Ÿä¸€çš„å·¥å…·è°ƒç”¨æ¥å£
                            return self._invoke_any_tool(tool_instance.name, prefer_mcp=True, **kwargs)
                        except Exception as e:
                            logger.error(f"MCPå·¥å…·è°ƒç”¨å¤±è´¥ {tool_instance.name}: {e}")
                            raise
                    
                    tool_task.__name__ = f"mcp_{tool_instance.name}"
                    tool_task.__doc__ = tool_instance.description
                    return tool_task
                
                # æ£€æŸ¥å·¥å…·æ˜¯å¦é€‚åˆå½“å‰æ™ºèƒ½ä½“è§’è‰²
                if self._tool_matches_role(tool):
                    wrapper_func = create_tool_wrapper(tool)
                    available_tools[f"mcp_{tool.name}"] = wrapper_func
                    logger.debug(f"æ™ºèƒ½ä½“ {self.name} æ·»åŠ MCPå·¥å…·: {tool.name}")
        

        
        logger.info(f"æ™ºèƒ½ä½“ {self.name} æ€»å…±å¯ç”¨å·¥å…·æ•°: {len(available_tools)} "
                   f"(MCPå¯ç”¨: {self._is_mcp_enabled()})")
        
        return available_tools
    
    def _capability_matches_role(self, capability_type: CapabilityType) -> bool:
        """æ£€æŸ¥èƒ½åŠ›ç±»å‹æ˜¯å¦åŒ¹é…æ™ºèƒ½ä½“è§’è‰²"""
        role_capability_mapping = {
            "data_processing": [CapabilityType.DATA_PROCESSING, CapabilityType.TOOL],
            "expert_analysis": [CapabilityType.ANALYSIS, CapabilityType.TOOL],
            "visualization": [CapabilityType.VISUALIZATION, CapabilityType.TOOL],
            "supervisor": [CapabilityType.TOOL, CapabilityType.ANALYSIS, CapabilityType.DATA_PROCESSING],
            # æ–°å¢åœ°çƒç‰©ç†åˆ†æç›¸å…³è§’è‰²
            "geophysics_analysis": [CapabilityType.ANALYSIS, CapabilityType.DATA_PROCESSING, CapabilityType.VISUALIZATION, CapabilityType.TOOL],
            "reservoir_engineering": [CapabilityType.ANALYSIS, CapabilityType.DATA_PROCESSING, CapabilityType.TOOL],
            "economic_evaluation": [CapabilityType.ANALYSIS, CapabilityType.TOOL],
            "quality_assurance": [CapabilityType.DATA_PROCESSING, CapabilityType.TOOL],
            "general_analysis": [CapabilityType.ANALYSIS, CapabilityType.DATA_PROCESSING, CapabilityType.VISUALIZATION, CapabilityType.TOOL]
        }
        
        allowed_types = role_capability_mapping.get(self.role, [CapabilityType.TOOL])
        return capability_type in allowed_types
    
    def _tool_matches_role(self, tool) -> bool:
        """æ£€æŸ¥MCPå·¥å…·æ˜¯å¦åŒ¹é…æ™ºèƒ½ä½“è§’è‰²"""
        # è·å–å·¥å…·çš„å…ƒæ•°æ®
        tool_registry = get_tool_registry()
        tool_metadata = tool_registry.get_tool_metadata(tool.name)
        
        if not tool_metadata:
            # å¦‚æœæ²¡æœ‰å…ƒæ•°æ®ï¼Œé»˜è®¤å…è®¸æ‰€æœ‰è§’è‰²ä½¿ç”¨
            return True
        
        tool_category = tool_metadata.get("category", "")
        
        # è§’è‰²ä¸å·¥å…·åˆ†ç±»çš„åŒ¹é…è§„åˆ™
        role_category_mapping = {
            "data_processing": ["file", "data", "isotope", "mcp"],
            "expert_analysis": ["analysis", "isotope", "knowledge", "mcp"],
            "visualization": ["visualization", "data", "file", "mcp"],
            "supervisor": ["file", "data", "analysis", "knowledge", "isotope", "mcp"],
            # ä¸“ä¸šè§’è‰²
            "geophysics_analysis": ["isotope", "data", "analysis", "visualization", "mcp"],
            "reservoir_engineering": ["data", "analysis", "isotope", "mcp"],
            "economic_evaluation": ["analysis", "data", "mcp"],
            "quality_assurance": ["data", "file", "analysis", "mcp"],
            "general_analysis": ["file", "data", "analysis", "knowledge", "isotope", "visualization", "mcp"],
            # ä¸­æ–‡è§’è‰²æ˜ å°„ï¼ˆæ–°å¢ï¼‰
            "å½•äº•èµ„æ–™å¤„ç†ä¸“å®¶": ["iso_logging", "gas_logging", "file", "data", "analysis", "mcp"],
            "åœ°éœ‡æ•°æ®å¤„ç†ä¸“å®¶": ["data", "analysis", "visualization", "mcp"],
            "ç³»ç»ŸåŠ©æ‰‹ä¸å’¨è¯¢ä¸“å®¶": ["file", "knowledge", "data", "analysis", "mcp"]
        }
        
        allowed_categories = role_category_mapping.get(self.role, ["mcp"])
        
        # æ£€æŸ¥å·¥å…·åˆ†ç±»æ˜¯å¦åŒ¹é…
        if tool_category in allowed_categories:
            return True
        
        # æ£€æŸ¥å·¥å…·åç§°æ˜¯å¦åŒ…å«è§’è‰²ç›¸å…³å…³é”®è¯
        tool_name_lower = tool.name.lower()
        role_keywords = {
            "data_processing": ["data", "file", "read", "write", "process"],
            "expert_analysis": ["analysis", "analyze", "isotope", "interpret"],
            "visualization": ["plot", "chart", "visual", "graph", "image"],
            "geophysics_analysis": ["isotope", "carbon", "data", "analysis"],
            "reservoir_engineering": ["reservoir", "pressure", "flow", "simulation"],
            "economic_evaluation": ["economic", "cost", "npv", "irr", "evaluation"],
            "quality_assurance": ["validate", "check", "quality", "verify"],
            "general_analysis": ["analysis", "data", "file", "isotope"],
            # ä¸­æ–‡è§’è‰²å…³é”®è¯ï¼ˆæ–°å¢ï¼‰
            "å½•äº•èµ„æ–™å¤„ç†ä¸“å®¶": ["isotope", "logging", "gas", "carbon", "analysis", "plot", "enhanced"],
            "åœ°éœ‡æ•°æ®å¤„ç†ä¸“å®¶": ["seismic", "data", "analysis", "visualization", "processing"],
            "ç³»ç»ŸåŠ©æ‰‹ä¸å’¨è¯¢ä¸“å®¶": ["file", "search", "knowledge", "rag", "preview", "query"]
        }
        
        keywords = role_keywords.get(self.role, [])
        for keyword in keywords:
            if keyword in tool_name_lower:
                return True
        
        # é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æœæ˜¯MCPå·¥å…·ä¸”æ²¡æœ‰æ˜ç¡®é™åˆ¶ï¼Œå…è®¸ä½¿ç”¨
        return tool_category == "mcp"
    
    def _is_mcp_enabled(self) -> bool:
        """åŠ¨æ€æ£€æŸ¥MCPæ˜¯å¦å¯ç”¨ï¼Œé¿å…å¾ªç¯å¯¼å…¥"""
        try:
            from app.tools.registry import is_mcp_enabled
            return is_mcp_enabled()
        except ImportError:
            return False
    
    def _get_mixed_tools(self):
        """åŠ¨æ€è·å–æ··åˆå·¥å…·ï¼Œé¿å…å¾ªç¯å¯¼å…¥"""
        try:
            from app.tools.registry import get_mixed_tools
            return get_mixed_tools()
        except ImportError:
            return []
    
    def _invoke_any_tool(self, tool_name: str, prefer_mcp: bool = False, **kwargs):
        """åŠ¨æ€è°ƒç”¨ä»»ä½•å·¥å…·ï¼Œé¿å…å¾ªç¯å¯¼å…¥"""
        try:
            from app.tools.registry import invoke_any_tool
            return invoke_any_tool(tool_name, prefer_mcp=prefer_mcp, **kwargs)
        except ImportError:
            raise RuntimeError(f"æ— æ³•å¯¼å…¥ invoke_any_tool å‡½æ•°: {tool_name}")
    
    def _fallback_task_identification(self, response_content: str) -> List[Dict[str, Any]]:
        """
        é™çº§å¤„ç†ï¼šå½“JSONè§£æå¤±è´¥æ—¶ï¼ŒåŸºäºå…³é”®è¯è¯†åˆ«ä»»åŠ¡
        
        Args:
            response_content: LLMçš„åŸå§‹å“åº”å†…å®¹
            
        Returns:
            è¯†åˆ«å‡ºçš„ä»»åŠ¡åˆ—è¡¨
        """
        fallback_tasks = []
        
        # åŸºäºå…³é”®è¯åŒ¹é…ä»»åŠ¡
        keyword_task_mapping = {
            "åˆ†æ": ["task_carbon_isotope_analysis", "task_isotope_data_analysis"],
            "å¯è§†åŒ–": ["task_create_visualization", "task_plot_data"],
            "æœç´¢": ["task_search_files", "task_search_knowledge"],
            "å¤„ç†": ["task_process_data", "task_clean_data"],
            "è®¡ç®—": ["task_calculate_statistics", "task_compute_metrics"],
            "è¯»å–": ["task_read_file", "task_load_data"],
            "æŠ¥å‘Š": ["task_generate_report", "task_create_summary"]
        }
        
        response_lower = response_content.lower()
        
        for keyword, task_names in keyword_task_mapping.items():
            if keyword in response_lower:
                for task_name in task_names:
                    if task_name in self._available_tools:
                        fallback_tasks.append({
                            "task_name": task_name,
                            "parameters": {},
                            "source": "keyword_fallback"
                        })
                        break  # æ¯ä¸ªå…³é”®è¯åªåŒ¹é…ä¸€ä¸ªä»»åŠ¡
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ä»»ä½•ä»»åŠ¡ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤çš„é€šç”¨ä»»åŠ¡
        if not fallback_tasks and self._available_tools:
            first_task = list(self._available_tools.keys())[0]
            fallback_tasks.append({
                "task_name": first_task,
                "parameters": {},
                "source": "default_fallback"
            })
        
        return fallback_tasks
    
    def _extract_json_from_response(self, response_content: str) -> str:
        """ä»LLMå“åº”ä¸­æå–JSONå†…å®¹"""
        import re
        
        # å°è¯•å¤šç§JSONæå–æ¨¡å¼
        patterns = [
            r'```json\s*(.*?)\s*```',  # ```json ... ```
            r'```\s*(.*?)\s*```',      # ``` ... ```
            r'`(.*?)`',                # `...`
            r'(\{.*\})',               # ç›´æ¥çš„JSONå¯¹è±¡
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, response_content, re.DOTALL | re.IGNORECASE)
            if matches:
                json_content = matches[0].strip()
                # éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆJSON
                try:
                    import json
                    json.loads(json_content)
                    return json_content
                except json.JSONDecodeError:
                    continue
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONæ¨¡å¼ï¼Œè¿”å›åŸå§‹å†…å®¹
        return response_content.strip()
    
    def _build_graph(self) -> StateGraph:
        """æ„å»ºæ™ºèƒ½ä½“çš„LangGraphå­å›¾"""
        # åˆ›å»ºçŠ¶æ€å›¾
        workflow = StateGraph(IsotopeSystemState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node(self.name + "_analyze", self._analyze_request)
        workflow.add_node(self.name + "_execute_task", self._execute_task)
        workflow.add_node(self.name + "_respond", self._generate_response)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point(self.name + "_analyze")
        
        # æ·»åŠ è¾¹
        workflow.add_edge(self.name + "_analyze", self.name + "_execute_task")
        workflow.add_edge(self.name + "_execute_task", self.name + "_respond")
        workflow.add_edge(self.name + "_respond", END)
        
        # ç¼–è¯‘å›¾
        return workflow.compile()
    
    def _analyze_request(self, state: IsotopeSystemState) -> IsotopeSystemState:
        """åˆ†æè¯·æ±‚å¹¶å†³å®šæ‰§è¡Œå“ªäº›ä»»åŠ¡"""
        # *** ä¿®å¤ï¼šå®‰å…¨è·å–æµå¼å†™å…¥å™¨ï¼Œé¿å…ä¸Šä¸‹æ–‡é”™è¯¯ ***

        push_thinking(agent_name=self.name, content=f"ğŸ” {self.name} æ­£åœ¨åˆ†ææ‚¨çš„è¯·æ±‚...", thinking_type="analysis")
        # è·å–æœ€æ–°çš„ç”¨æˆ·æ¶ˆæ¯
        last_human_msg = StateManager.get_last_human_message(state)
        if not last_human_msg:
            return state
        
        # ä½¿ç”¨è®°å¿†ç³»ç»Ÿå¢å¼ºåˆ†æ
        enhanced_prompt = self._enhance_prompt_with_memories(last_human_msg.content, state)
        
        # è·å–å¯ç”¨æ–‡ä»¶ä¿¡æ¯ï¼Œç”¨äºä»»åŠ¡å‚æ•°ï¼ˆæ”¯æŒå­—å…¸å’Œåˆ—è¡¨æ ¼å¼ï¼‰
        available_files = state.get("files", [])
        file_list = []
        
        if isinstance(available_files, dict):
            # å¦‚æœæ˜¯å­—å…¸æ ¼å¼ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨
            for file_id, file_info in available_files.items():
                file_name = file_info.get("name", file_id)
                file_type = file_info.get("type", "æœªçŸ¥")
                file_size = file_info.get("size", 0)
                file_list.append(f"  - {file_name} (ID: {file_id}, ç±»å‹: {file_type}, å¤§å°: {file_size})")
        elif isinstance(available_files, list):
            # å¦‚æœæ˜¯åˆ—è¡¨æ ¼å¼ï¼Œç›´æ¥ä½¿ç”¨
            for file_info in available_files:
                file_id = file_info.get("file_id", "æœªçŸ¥")
                file_name = file_info.get("name", file_id)
                file_type = file_info.get("type", "æœªçŸ¥")
                file_size = file_info.get("size", 0)
                file_list.append(f"  - {file_name} (ID: {file_id}, ç±»å‹: {file_type}, å¤§å°: {file_size})")
        
        # æ„å»ºå·¥å…·æè¿°
        available_tools_desc = []
        for tool_name, tool_func in self._available_tools.items():
            # è·å–å·¥å…·çš„æ–‡æ¡£å­—ç¬¦ä¸²æˆ–æè¿°
            tool_desc = getattr(tool_func, "__doc__", "").strip() if hasattr(tool_func, "__doc__") else ""
            if not tool_desc:
                tool_desc = f"æ‰§è¡Œ{tool_name}å·¥å…·"
            available_tools_desc.append(f"  - {tool_name}: {tool_desc}")
        
        # æ„å»ºåŸºç¡€æç¤ºè¯
        base_prompt = f"""ä½ æ˜¯{self.name}ï¼Œè§’è‰²æ˜¯{self.role}ã€‚

ç”¨æˆ·è¯·æ±‚: {last_human_msg.content}

å¯ç”¨æ–‡ä»¶:
{chr(10).join(file_list) if file_list else "- æš‚æ— å¯ç”¨æ–‡ä»¶"}

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»»åŠ¡:
{chr(10).join(available_tools_desc)}

è¯·åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œå†³å®šéœ€è¦æ‰§è¡Œå“ªäº›ä»»åŠ¡ã€‚å¦‚æœä»»åŠ¡éœ€è¦æ–‡ä»¶IDå‚æ•°ï¼Œè¯·ä½¿ç”¨ä¸Šé¢åˆ—å‡ºçš„å®é™…æ–‡ä»¶IDã€‚

**é‡è¦ï¼šå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š**

```json
{{
    "tasks_to_execute": [
        {{"task_name": "ä»»åŠ¡åç§°", "parameters": {{"file_id": "å®é™…çš„æ–‡ä»¶ID"}}}},
        {{"task_name": "å¦ä¸€ä¸ªä»»åŠ¡åç§°", "parameters": {{}}}}
    ],
    "reasoning": "ä½ çš„åˆ†æå’Œæ¨ç†è¿‡ç¨‹"
}}
```

æ³¨æ„ï¼š
1. åªè¾“å‡ºJSONï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡Šæ–‡å­—
2. task_nameå¿…é¡»ä»ä¸Šé¢çš„å¯ç”¨ä»»åŠ¡åˆ—è¡¨ä¸­é€‰æ‹©
3. å¦‚æœéœ€è¦file_idå‚æ•°ï¼Œè¯·ä½¿ç”¨å®é™…çš„æ–‡ä»¶IDï¼Œä¸è¦ä½¿ç”¨å ä½ç¬¦
4. å¦‚æœä¸éœ€è¦æ‰§è¡Œä»»ä½•ä»»åŠ¡ï¼Œtasks_to_executeè®¾ä¸ºç©ºæ•°ç»„[]"""
        
        # ä½¿ç”¨è®°å¿†å¢å¼ºçš„æç¤ºè¯
        final_prompt = enhanced_prompt if enhanced_prompt else base_prompt
        
        # è°ƒç”¨LLM
        response = self.llm.invoke([HumanMessage(content=final_prompt)])
        
        # è§£æJSONå“åº”
        try:
            # æå–JSONå†…å®¹
            json_content = self._extract_json_from_response(response.content)
            
            # è§£æJSON
            analysis = json.loads(json_content)
            
            # éªŒè¯JSONç»“æ„
            if not isinstance(analysis, dict) or "tasks_to_execute" not in analysis:
                raise ValueError("JSONæ ¼å¼ä¸æ­£ç¡®")
                
            # ä¿å­˜åˆ†æç»“æœ
            state["agent_analysis"] = {
                "agent": self.name,
                "tasks": analysis.get("tasks_to_execute", []),
                "reasoning": analysis.get("reasoning", ""),
                "analysis_success": True
            }
            
            # é€šçŸ¥ç”¨æˆ·ä»»åŠ¡åˆ†æç»“æœ
            task_count = len(analysis.get("tasks_to_execute", []))
            
            if task_count > 0:
                task_names = [task.get("task_name", "") for task in analysis.get("tasks_to_execute", [])]
                push_thinking(agent_name=self.name, content=f"ğŸ“‹ åˆ†æå®Œæˆï¼è¯†åˆ«å‡º {task_count} ä¸ªç›¸å…³ä»»åŠ¡ï¼š{', '.join(task_names[:3])}{'...' if len(task_names) > 3 else ''}", thinking_type="analysis")
                
                # è®°å½•è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
                logger.info(f"âœ… {self.name} ä»»åŠ¡åˆ†ææˆåŠŸï¼šè¯†åˆ«åˆ° {task_count} ä¸ªä»»åŠ¡")
                for i, task in enumerate(analysis.get("tasks_to_execute", [])):
                    logger.info(f"  ä»»åŠ¡ {i+1}: {task.get('task_name')} - å‚æ•°: {task.get('parameters', {})}")
            else:
                push_thinking(agent_name=self.name, content=f"ğŸ’­ ç†è§£äº†æ‚¨çš„éœ€æ±‚ï¼Œæ­£åœ¨å‡†å¤‡ç›¸åº”çš„å›å¤", thinking_type="analysis")
                logger.info(f"â„¹ï¸ {self.name} ä»»åŠ¡åˆ†æï¼šæ— éœ€æ‰§è¡Œç‰¹å®šä»»åŠ¡")
            
            # ä¿å­˜è®°å¿†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.use_memory and self.memory_integration:
                self._save_analysis_to_memory(state, analysis)
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥: {e}")
            logger.error(f"å“åº”å†…å®¹: {response.content}")
            
            # é™çº§å¤„ç†ï¼šåŸºäºå…³é”®è¯è¯†åˆ«ä»»åŠ¡
            fallback_tasks = self._fallback_task_identification(response.content)
            
            state["agent_analysis"] = {
                "agent": self.name,
                "tasks": fallback_tasks,
                "reasoning": f"ä½¿ç”¨æ™ºèƒ½åˆ†æè¯†åˆ«ä»»åŠ¡",
                "fallback": True,
                "error": str(e),
                "analysis_success": False
            }
            
            push_thinking(agent_name=self.name, content=f"ğŸ”„ JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨æ™ºèƒ½åˆ†æè¯†åˆ«åˆ° {len(fallback_tasks)} ä¸ªç›¸å…³ä»»åŠ¡", thinking_type="analysis")
            
        except Exception as e:
            logger.error(f"åˆ†æè¯·æ±‚å¤±è´¥: {e}")
            state["agent_analysis"] = {
                "agent": self.name,
                "tasks": [],
                "error": str(e),
                "analysis_success": False
            }
            
            push_thinking(agent_name=self.name, content=f"âš ï¸ åˆ†æè¿‡ç¨‹é‡åˆ°é—®é¢˜ï¼Œå°†æä¾›é€šç”¨å›å¤", thinking_type="analysis")
        
        return state
    
    def _execute_task(self, state: IsotopeSystemState) -> IsotopeSystemState:
        """æ‰§è¡Œè¯†åˆ«å‡ºçš„ä»»åŠ¡"""
        # *** ä¿®å¤ï¼šå®‰å…¨è·å–æµå¼å†™å…¥å™¨ ***

        
        # *** å…³é”®ä¿®å¤ï¼šç›´æ¥ä»åŸå§‹çŠ¶æ€è¯»å–ä»»åŠ¡ä¿¡æ¯ ***
        analysis = state.get("agent_analysis", {})
        tasks = analysis.get("tasks", [])
        
        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
        logger.info(f"[DEBUG] _execute_task - æ™ºèƒ½ä½“: {self.name}")
        logger.info(f"[DEBUG] state keys: {list(state.keys())}")
        logger.info(f"[DEBUG] agent_analysis: {analysis}")
        logger.info(f"[DEBUG] tasks count: {len(tasks)}")
        logger.info(f"[DEBUG] available_tools count: {len(self._available_tools)}")
        logger.info(f"[DEBUG] available_tools: {list(self._available_tools.keys())}")
        
        # *** ä¿®å¤ï¼šæ£€æŸ¥åˆ†ææ˜¯å¦æˆåŠŸ ***
        if analysis.get("analysis_success", False):
            logger.info(f"âœ… {self.name} æˆåŠŸè·å–ä»»åŠ¡åˆ†æç»“æœ")
        else:
            logger.warning(f"âš ï¸ {self.name} ä»»åŠ¡åˆ†æå¯èƒ½å¤±è´¥æˆ–ä¸å®Œæ•´")
        
        if not tasks:
            # *** ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦æœ‰ä»»åŠ¡ä½†å› ä¸ºçŠ¶æ€ä¼ é€’é—®é¢˜ä¸¢å¤±äº† ***
            if analysis:
                logger.warning(f"âš ï¸ æœ‰åˆ†æç»“æœä½†æ— ä»»åŠ¡ï¼š{analysis}")
                push_thinking(agent_name=self.name, content=f"ğŸ’­ åˆ†æå®Œæˆä½†æ— éœ€æ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼Œå°†ç›´æ¥ç”Ÿæˆå›å¤", thinking_type="analysis")
            else:
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ä»»åŠ¡åˆ†æç»“æœï¼Œå¯èƒ½æ˜¯çŠ¶æ€ä¼ é€’é—®é¢˜")
            push_thinking(agent_name=self.name, content=f"ğŸ’­ æ— éœ€æ‰§è¡Œç‰¹å®šä»»åŠ¡ï¼Œå°†ç›´æ¥ç”Ÿæˆå›å¤", thinking_type="analysis")
            push_thinking(agent_name=self.name, content=f"[è°ƒè¯•] åŸå› ï¼šæ— åˆ†æç»“æœæˆ–ä»»åŠ¡åˆ—è¡¨ä¸ºç©º", thinking_type="analysis")
            return state
        
        # åˆå§‹åŒ–ä»»åŠ¡æ‰§è¡Œç»“æœ
        if "task_results" not in state or state["task_results"] is None:
            state["task_results"] = []
        
        push_thinking(agent_name=self.name, content=f"ğŸ”„ å¼€å§‹æ‰§è¡Œ {len(tasks)} ä¸ªä»»åŠ¡...", thinking_type="analysis")
        
        # æ‰§è¡Œæ¯ä¸ªä»»åŠ¡
        success_count = 0
        for i, task_info in enumerate(tasks, 1):
            task_name = task_info.get("task_name")
            parameters = task_info.get("parameters", {})
            
            if task_name not in self._available_tools:
                logger.warning(f"ä»»åŠ¡ {task_name} ä¸å¯ç”¨")
                push_thinking(agent_name=self.name, content=f"âš ï¸ ä»»åŠ¡ {task_name} ä¸åœ¨å¯ç”¨ä»»åŠ¡åˆ—è¡¨ä¸­ï¼Œè·³è¿‡", thinking_type="analysis")
                continue
            
            push_thinking(agent_name=self.name, content=f"ğŸš€ æ­£åœ¨æ‰§è¡Œä»»åŠ¡ {i}/{len(tasks)}: {task_name}", thinking_type="analysis")
            
            # *** å…³é”®ä¿®å¤ï¼šè®°å½•ä»»åŠ¡æ‰§è¡Œå¼€å§‹æ—¶é—´ ***
            start_time = time.time()
            
            try:
                # è·å–ä»»åŠ¡å‡½æ•°
                task_func = self._available_tools[task_name]
                
                logger.info(f"â±ï¸ å¼€å§‹æ‰§è¡Œä»»åŠ¡ {task_name}ï¼Œå‚æ•°: {parameters}")
                
                # *** å…³é”®ä¿®å¤ï¼šåœ¨LangGraphä¸Šä¸‹æ–‡ä¸­æ‰§è¡Œä»»åŠ¡ï¼Œç¡®ä¿æµå¼è¾“å‡ºä¼ é€’ ***
                # å¦‚æœtaskå‡½æ•°æœ‰_task_configå±æ€§ï¼Œè¯´æ˜å®ƒæ˜¯è¢«@taskè£…é¥°çš„
                if hasattr(task_func, '_task_config'):
                    # å°è¯•åº”ç”¨LangGraphè£…é¥°å™¨æ¥ç¡®ä¿æµå¼è¾“å‡ºæ­£ç¡®ä¼ é€’
                    from app.core.task_decorator import apply_langgraph_decorator
                    try:
                        # åœ¨å½“å‰LangGraphä¸Šä¸‹æ–‡ä¸­åº”ç”¨è£…é¥°å™¨
                        enhanced_task_func = apply_langgraph_decorator(task_func)
                        result = enhanced_task_func(**parameters)
                    except Exception as decorator_error:
                        logger.warning(f"LangGraphè£…é¥°å™¨åº”ç”¨å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å‡½æ•°: {decorator_error}")
                        # ç›´æ¥æ‰§è¡ŒåŸå§‹ä»»åŠ¡å‡½æ•°
                        result = task_func(**parameters)
                else:
                    # ç›´æ¥æ‰§è¡Œä»»åŠ¡å‡½æ•°
                    result = task_func(**parameters)
                
                # *** å…³é”®ä¿®å¤ï¼šè®°å½•ä»»åŠ¡æ‰§è¡Œæ—¶é—´ ***
                execution_time = time.time() - start_time
                logger.info(f"âœ… ä»»åŠ¡ {task_name} æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
                
                # *** å…³é”®ä¿®å¤ï¼šè¯¦ç»†è®°å½•æ‰§è¡Œç»“æœï¼Œç¡®ä¿å¯åºåˆ—åŒ– ***
                # ç¡®ä¿resultæ˜¯å¯åºåˆ—åŒ–çš„ï¼Œé¿å…Futureç­‰ä¸å¯åºåˆ—åŒ–å¯¹è±¡
                serializable_result = result
                if hasattr(result, '__dict__'):
                    try:
                        # å°è¯•è½¬æ¢ä¸ºå­—ç¬¦ä¸²å½¢å¼ï¼Œé¿å…å¤æ‚å¯¹è±¡
                        serializable_result = str(result)
                    except Exception:
                        serializable_result = f"<{type(result).__name__} object>"
                
                execution_record = {
                    "task_name": task_name,
                    "parameters": parameters,
                    "result": serializable_result,  # ä¿å­˜å¯åºåˆ—åŒ–çš„ç»“æœå†…å®¹
                    "execution_time": execution_time,
                    "status": "success",
                    "timestamp": datetime.now().isoformat()
                }
                
                state["task_results"].append(execution_record)
                success_count += 1
                
                push_thinking(agent_name=self.name, content=f"âœ… ä»»åŠ¡ {task_name} æ‰§è¡ŒæˆåŠŸ", thinking_type="analysis")
                # *** å…³é”®ä¿®å¤ï¼šæä¾›æ›´è¯¦ç»†çš„å·¥å…·è¾“å‡ºä¿¡æ¯ ***
                push_progress({
                        "tool_name": task_name,
                        "progress": 100,
                        "details": str(result)[:500] + "..." if len(str(result)) > 500 else str(result),
                        "source": "task"
                    })
                
            except Exception as e:
                execution_time = time.time() - start_time
                logger.error(f"æ‰§è¡Œä»»åŠ¡ {task_name} å¤±è´¥: {e}")
                logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸è¯¦æƒ…: {traceback.format_exc()}")
                
                execution_record = {
                    "task_name": task_name,
                    "parameters": parameters,
                    "error": str(e),
                    "execution_time": execution_time,
                    "status": "failed",
                    "timestamp": datetime.now().isoformat()
                }
                
                state["task_results"].append(execution_record)
                
                push_thinking(agent_name=self.name, content=f"âŒ ä»»åŠ¡ {task_name} æ‰§è¡Œå¤±è´¥: {str(e)[:100]}...", thinking_type="analysis")
                push_progress({
                        "tool_name": task_name,
                        "progress": 0,
                        "details": str(e)[:500] + "..." if len(str(e)) > 500 else str(e),
                        "source": "task"
                    })
        
        # æ€»ç»“æ‰§è¡Œç»“æœ
        if success_count == len(tasks):
            push_thinking(agent_name=self.name, content=f"ğŸ‰ æ‰€æœ‰ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼æˆåŠŸå®Œæˆ {success_count} ä¸ªä»»åŠ¡", thinking_type="analysis")
        elif success_count > 0:
            push_thinking(agent_name=self.name, content=f"âš¡ ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {len(tasks) - success_count} ä¸ª", thinking_type="analysis")
        else:
            push_thinking(agent_name=self.name, content=f"âš ï¸ ä»»åŠ¡æ‰§è¡Œé‡åˆ°é—®é¢˜ï¼Œæ‰€æœ‰ä»»åŠ¡éƒ½æœªèƒ½æˆåŠŸå®Œæˆ", thinking_type="analysis")
        
        # *** å…³é”®ä¿®å¤ï¼šè®°å½•æœ€ç»ˆçš„ä»»åŠ¡æ‰§è¡Œç»Ÿè®¡ ***
        logger.info(f"ğŸ“Š ä»»åŠ¡æ‰§è¡Œç»Ÿè®¡ - æ€»æ•°: {len(tasks)}, æˆåŠŸ: {success_count}, å¤±è´¥: {len(tasks) - success_count}")
        
        return state
    
    def _generate_response(self, state: IsotopeSystemState) -> IsotopeSystemState:
        """åŸºäºä»»åŠ¡æ‰§è¡Œç»“æœç”Ÿæˆå“åº”"""
        # *** ä¿®å¤ï¼šå®‰å…¨è·å–æµå¼å†™å…¥å™¨ ***
        push_thinking(agent_name=self.name, content=f"ğŸ“ {self.name} æ­£åœ¨æ•´ç†åˆ†æç»“æœå¹¶ç”Ÿæˆå›å¤...", thinking_type="analysis")
        
        # è·å–ä»»åŠ¡æ‰§è¡Œç»“æœ
        task_results = state.get("task_results", [])
        analysis = state.get("agent_analysis", {})
        
        # è·å–ç”¨æˆ·åŸå§‹è¯·æ±‚
        last_human_msg = StateManager.get_last_human_message(state)
        user_request = last_human_msg.content if last_human_msg else "ç”¨æˆ·è¯·æ±‚"
        
        # å‡†å¤‡ç»“æœæ‘˜è¦
        success_tasks = [r for r in task_results if r["status"] == "success"]
        failed_tasks = [r for r in task_results if r["status"] == "failed"]
        
        results_summary = []
        detailed_results = []  # *** æ–°å¢ï¼šå­˜å‚¨è¯¦ç»†çš„ä»»åŠ¡ç»“æœ ***
        
        if success_tasks:
            results_summary.append("âœ… æˆåŠŸå®Œæˆçš„ä»»åŠ¡:")
            for result in success_tasks:
                execution_time = result.get('execution_time', 0)
                results_summary.append(f"  â€¢ {result['task_name']} (è€—æ—¶: {execution_time:.2f}ç§’)")
                
                # *** å…³é”®ä¿®å¤ï¼šæå–taskçš„å…·ä½“ç»“æœå†…å®¹ ***
                task_result_content = result.get('result', '')
                if task_result_content and isinstance(task_result_content, str):
                    # é™åˆ¶æ¯ä¸ªä»»åŠ¡ç»“æœçš„é•¿åº¦ï¼Œé¿å…æç¤ºè¯è¿‡é•¿
                    max_result_length = 2000
                    if len(task_result_content) > max_result_length:
                        task_result_summary = task_result_content[:max_result_length] + "...(ç»“æœå·²æˆªæ–­)"
                    else:
                        task_result_summary = task_result_content
                    
                    detailed_results.append(f"""
**ä»»åŠ¡ {result['task_name']} çš„æ‰§è¡Œç»“æœ:**
å‚æ•°: {result.get('parameters', {})}
æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’
ç»“æœå†…å®¹:
{task_result_summary}
""")
        
        if failed_tasks:
            results_summary.append("âŒ æ‰§è¡Œå¤±è´¥çš„ä»»åŠ¡:")
            for result in failed_tasks:
                error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')[:50]
                execution_time = result.get('execution_time', 0)
                results_summary.append(f"  â€¢ {result['task_name']} - {error_msg}... (è€—æ—¶: {execution_time:.2f}ç§’)")
        
        # *** å…³é”®ä¿®å¤ï¼šåŸºäºå…·ä½“çš„ä»»åŠ¡ç»“æœå†…å®¹ç”Ÿæˆå›å¤ ***
        if not task_results:
            prompt = f"""ç”¨æˆ·è¯·æ±‚: {user_request}

ä½œä¸º{self.name}ï¼ˆ{self.role}ï¼‰ï¼Œè¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

è¯·ç”¨ä¸“ä¸šã€å‹å¥½çš„è¯­è¨€å›å¤ï¼Œç¡®ä¿ï¼š
1. ç›´æ¥è§£ç­”ç”¨æˆ·çš„ç–‘é—®
2. å¦‚æœæ˜¯ä¸“ä¸šé—®é¢˜ï¼Œæä¾›å‡†ç¡®çš„æŠ€æœ¯ä¿¡æ¯  
3. å¦‚æœæ˜¯å’¨è¯¢ç±»é—®é¢˜ï¼Œç»™å‡ºæ¸…æ™°çš„æŒ‡å¯¼
4. è¯­æ°”è‡ªç„¶ï¼Œé¿å…è¿‡äºæœºæ¢°åŒ–
"""
        else:
            # *** å…³é”®ä¿®å¤ï¼šåŸºäºè¯¦ç»†çš„ä»»åŠ¡æ‰§è¡Œç»“æœç”Ÿæˆå“åº” ***
            detailed_results_text = "\n".join(detailed_results)
            
            prompt = f"""ç”¨æˆ·è¯·æ±‚: {user_request}

ä»»åŠ¡æ‰§è¡Œæ¦‚è¦:
{chr(10).join(results_summary)}

è¯¦ç»†ä»»åŠ¡æ‰§è¡Œç»“æœ:
{detailed_results_text}

ä½œä¸ºä¸“ä¸šçš„{self.role}ï¼Œè¯·åŸºäºä»¥ä¸Šä»»åŠ¡çš„å…·ä½“æ‰§è¡Œç»“æœï¼Œç”Ÿæˆä¸€ä¸ªå…¨é¢ã€ä¸“ä¸šçš„å›å¤ã€‚è¦æ±‚ï¼š

1. **å‡†ç¡®ç†è§£ç”¨æˆ·éœ€æ±‚**ï¼šé¦–å…ˆç¡®è®¤ç†è§£äº†ç”¨æˆ·çš„å…·ä½“è¯·æ±‚
2. **åŸºäºå®é™…ç»“æœå›å¤**ï¼šé‡ç‚¹åŸºäºä»»åŠ¡çš„å…·ä½“æ‰§è¡Œç»“æœå’Œåˆ†æå†…å®¹è¿›è¡Œå›å¤ï¼Œä¸è¦ç¼–é€ ä¿¡æ¯
3. **çªå‡ºå…³é”®å‘ç°**ï¼šå¦‚æœä»»åŠ¡äº§ç”Ÿäº†å…·ä½“çš„åˆ†æç»“è®ºã€æ•°æ®æ´å¯Ÿæˆ–å›¾è¡¨ï¼Œè¯·é‡ç‚¹è¯´æ˜
4. **ä¸“ä¸šæœ¯è¯­é€‚åº¦**ï¼šä½¿ç”¨ä¸“ä¸šä½†æ˜“æ‡‚çš„è¯­è¨€ï¼Œé¿å…è¿‡äºæŠ€æœ¯åŒ–çš„è¡¨è¿°
5. **å®ç”¨å»ºè®®**ï¼šå¦‚æœåˆé€‚ï¼Œæä¾›ä¸‹ä¸€æ­¥çš„æ“ä½œå»ºè®®æˆ–åˆ†ææ–¹å‘
6. **æ–‡ä»¶å’Œå›¾è¡¨è¯´æ˜**ï¼šå¦‚æœä»»åŠ¡ç”Ÿæˆäº†å›¾è¡¨æˆ–æ–‡ä»¶ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·å¦‚ä½•æŸ¥çœ‹å’Œåˆ©ç”¨è¿™äº›ç»“æœ

æ³¨æ„äº‹é¡¹ï¼š
- ä¸è¦ç›´æ¥å¤åˆ¶ç²˜è´´åŸå§‹ä»»åŠ¡æ‰§è¡Œæ•°æ®
- è¦ç”¨è‡ªç„¶è¯­è¨€æ€»ç»“å’Œè§£é‡Šå…³é”®ä¿¡æ¯
- å¦‚æœæœ‰å¤šä¸ªä»»åŠ¡ç»“æœï¼Œè¦ç»¼åˆåˆ†æå¹¶ç»™å‡ºæ•´ä½“ç»“è®º
- ä¿æŒå›å¤çš„é€»è¾‘æ€§å’Œä¸“ä¸šæ€§
- å¦‚æœæŸäº›ä»»åŠ¡å¤±è´¥ï¼Œè¦è¯šå®è¯´æ˜ä½†ä¸è¦è¿‡åˆ†å¼ºè°ƒæŠ€æœ¯ç»†èŠ‚
"""
        
        response = self.llm.invoke([HumanMessage(content=prompt)])
        
        # æä¾›æ›´å‹å¥½çš„å®Œæˆæç¤º
        if len(success_tasks) == len(task_results) and task_results:
            push_thinking(agent_name=self.name, content=f"ğŸ¯ åˆ†æå®Œæˆï¼ä¸ºæ‚¨ç”Ÿæˆäº†åŸºäºå®é™…ç»“æœçš„å…¨é¢åˆ†ææŠ¥å‘Š", thinking_type="analysis")
        elif success_tasks and failed_tasks:
            push_thinking(agent_name=self.name, content=f"ğŸ“Š åˆ†æåŸºæœ¬å®Œæˆï¼Œå·²åŸºäºæˆåŠŸä»»åŠ¡çš„ç»“æœç”ŸæˆæŠ¥å‘Š", thinking_type="analysis")
        elif not task_results:
            push_thinking(agent_name=self.name, content=f"ğŸ’¬ ä¸ºæ‚¨å‡†å¤‡äº†è¯¦ç»†çš„å›å¤", thinking_type="analysis")
        else:
            push_thinking(agent_name=self.name, content=f"âš¡ å·²ä¸ºæ‚¨ç”Ÿæˆå›å¤ï¼Œå¦‚éœ€æ›´è¯¦ç»†åˆ†æè¯·ä¸Šä¼ ç›¸å…³æ–‡ä»¶", thinking_type="analysis")
        
        # *** å…³é”®ä¿®å¤ï¼šåœ¨AIæ¶ˆæ¯ä¸­åŒ…å«æ›´å¤šçš„æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯ ***
        total_execution_time = sum(r.get('execution_time', 0) for r in task_results)
        
        ai_message = AIMessage(
            content=response.content,
            additional_kwargs={
                "source": self.name,
                "role": self.role,
                "tasks_executed": len(task_results),
                "success_count": len(success_tasks),
                "failed_count": len(failed_tasks),
                "total_execution_time": f"{total_execution_time:.2f}ç§’",
                "has_detailed_results": len(detailed_results) > 0,
                "task_names": [r['task_name'] for r in success_tasks]
            }
        )
        
        # æ›´æ–°æ¶ˆæ¯å†å²
        state = StateManager.update_messages(state, ai_message)
        
        return state
    
    def run(self, state: IsotopeSystemState) -> IsotopeSystemState:
        """è¿è¡Œæ™ºèƒ½ä½“"""
        logger.info(f"LangGraphAgent {self.name} å¼€å§‹è¿è¡Œ")
        
        # *** å…³é”®ä¿®å¤ï¼šç¡®ä¿è¾“å…¥çŠ¶æ€å®Œæ•´ ***
        if not isinstance(state, dict):
            logger.error(f"âš ï¸ {self.name} æ¥æ”¶åˆ°çš„çŠ¶æ€ä¸æ˜¯å­—å…¸æ ¼å¼: {type(state)}")
            state = {}
        
        # è®°å¿†å¢å¼ºé¢„å¤„ç†
        if self.memory_integration:
            try:
                # æå–å’Œå­˜å‚¨è®°å¿†
                extracted_memories = self.memory_integration.extract_memories_from_state(
                    state, self.role
                )
                if extracted_memories:
                    logger.info(f"æ™ºèƒ½ä½“ {self.name} æå–åˆ° {len(extracted_memories)} æ¡è®°å¿†")
                
                # å¢å¼ºçŠ¶æ€
                memory_context = self.memory_integration.enhance_state_with_agent_memories(
                    state, self.role
                )
                
                # å°†è®°å¿†ä¸Šä¸‹æ–‡æ³¨å…¥åˆ°çŠ¶æ€ä¸­
                if hasattr(memory_context, 'memory_summary') and memory_context.memory_summary:
                    if 'metadata' not in state:
                        state['metadata'] = {}
                    state['metadata']['agent_memory_context'] = {
                        'summary': memory_context.memory_summary,
                        'confidence': memory_context.confidence_score,
                        'domain_coverage': memory_context.domain_coverage
                    }
                    logger.info(f"æ™ºèƒ½ä½“ {self.name} è®°å¿†å¢å¼ºå®Œæˆ")
                    
            except Exception as e:
                logger.warning(f"æ™ºèƒ½ä½“ {self.name} è®°å¿†å¢å¼ºå¤±è´¥: {str(e)}")
        
        # ä¿¡æ¯ä¸­æ¢æ—¥å¿—è®°å½•
        if self.info_hub:
            try:
                session_id = state.get('metadata', {}).get('session_id', 'default')
                self.info_hub.log_event(session_id, {
                    'event_type': 'agent_execution_start',
                    'agent_name': self.name,
                    'agent_role': self.role,
                    'timestamp': datetime.now().isoformat()
                })
            except Exception as e:
                logger.warning(f"ä¿¡æ¯ä¸­æ¢æ—¥å¿—è®°å½•å¤±è´¥: {str(e)}")
        
        # ç¡®ä¿çŠ¶æ€åŒ…å«å¿…è¦å­—æ®µï¼ˆä¿®å¤Noneå€¼é—®é¢˜ï¼‰
        if "agent_analysis" not in state or state["agent_analysis"] is None:
            state["agent_analysis"] = {}
        if "task_results" not in state or state["task_results"] is None:
            state["task_results"] = []
        
        try:
            # *** ä¿®å¤ï¼šè®°å½•è¿è¡Œå‰çš„çŠ¶æ€ ***
            logger.info(f"ğŸ“Š {self.name} è¿è¡Œå‰çŠ¶æ€æ£€æŸ¥:")
            logger.info(f"  - messages: {len(state.get('messages', []))}")
            logger.info(f"  - files: {len(state.get('files', {}))}")
            logger.info(f"  - agent_analysiså­˜åœ¨: {'agent_analysis' in state}")
            if 'agent_analysis' in state:
                analysis = state['agent_analysis']
                logger.info(f"  - analysiså†…å®¹: {analysis}")
                # *** å…³é”®ä¿®å¤ï¼šå¤„ç†analysisä¸ºNoneçš„æƒ…å†µ ***
                if analysis is not None and isinstance(analysis, dict):
                    logger.info(f"  - tasksæ•°é‡: {len(analysis.get('tasks', []))}")
                else:
                    logger.info(f"  - tasksæ•°é‡: 0 (analysisä¸ºNoneæˆ–éå­—å…¸æ ¼å¼)")
            
            # è¿è¡Œç¼–è¯‘åçš„å›¾
            result = self.graph.invoke(state)
            
            # *** ä¿®å¤ï¼šè®°å½•è¿è¡Œåçš„çŠ¶æ€ ***
            logger.info(f"ğŸ“Š {self.name} è¿è¡ŒåçŠ¶æ€æ£€æŸ¥:")
            logger.info(f"  - messages: {len(result.get('messages', []))}")
            logger.info(f"  - agent_analysiså­˜åœ¨: {'agent_analysis' in result}")
            if 'agent_analysis' in result:
                analysis = result['agent_analysis']
                logger.info(f"  - analysiså†…å®¹: {analysis}")
                # *** å…³é”®ä¿®å¤ï¼šå¤„ç†analysisä¸ºNoneçš„æƒ…å†µ ***
                if analysis is not None and isinstance(analysis, dict):
                    logger.info(f"  - åˆ†ææˆåŠŸ: {analysis.get('analysis_success', False)}")
                    logger.info(f"  - è¯†åˆ«çš„ä»»åŠ¡æ•°: {len(analysis.get('tasks', []))}")
                else:
                    logger.info(f"  - analysisä¸ºNoneæˆ–éå­—å…¸æ ¼å¼ï¼Œå¯èƒ½å­˜åœ¨çŠ¶æ€ä¼ é€’é—®é¢˜")
            
            return result
        except Exception as e:
            logger.error(f"LangGraphAgent {self.name} è¿è¡Œå¤±è´¥: {e}")
            logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            # æ·»åŠ é”™è¯¯æ¶ˆæ¯
            error_msg = AIMessage(
                content=f"æŠ±æ­‰ï¼Œ{self.name}åœ¨å¤„ç†è¯·æ±‚æ—¶é‡åˆ°é”™è¯¯: {str(e)}",
                additional_kwargs={"error": True, "source": self.name}
            )
            return StateManager.update_messages(state, error_msg)
    
    def _enhance_prompt_with_memories(self, user_query: str, state: IsotopeSystemState) -> Optional[str]:
        """ä½¿ç”¨è®°å¿†ç³»ç»Ÿå¢å¼ºæç¤ºè¯"""
        if not self.use_memory or not self.memory_injector:
            return None
            
        try:
            # æå–å½“å‰ç”¨æˆ·IDå’Œä¼šè¯ID
            user_id = state.get('metadata', {}).get('user_id', 'default_user')
            session_id = state.get('metadata', {}).get('session_id', 'default_session')
            
            # è·å–è®°å¿†ä¸Šä¸‹æ–‡
            memory_context = self.memory_integration.enhance_state_with_agent_memories(
                state, 
                agent_role=self.role,
                query=user_query
            )
            
            # å¦‚æœæ²¡æœ‰ç›¸å…³è®°å¿†ï¼Œè¿”å›None
            if memory_context.confidence_score < 0.3:
                return None
            
            # æ„å»ºè®°å¿†å¢å¼ºçš„æç¤ºè¯
            memory_section = self._format_memory_context(memory_context)
            
            # è·å–åŸºç¡€æç¤ºè¯ç»„ä»¶
            base_prompt = self._build_base_prompt(user_query, state)
            
            # æ³¨å…¥è®°å¿†åˆ°æç¤ºè¯ä¸­
            enhanced_prompt = f"""åŸºäºä»¥ä¸‹ç›¸å…³è®°å¿†ä¿¡æ¯å’Œå½“å‰è¯·æ±‚ï¼Œè¿›è¡Œåˆ†æï¼š

{memory_section}

---

{base_prompt}

è¯·ç»“åˆä¸Šè¿°è®°å¿†ä¿¡æ¯ï¼Œè¿›è¡Œæ›´å‡†ç¡®çš„ä»»åŠ¡åˆ†æã€‚"""
            
            logger.info(f"æ™ºèƒ½ä½“ {self.name} ä½¿ç”¨è®°å¿†å¢å¼ºæç¤ºè¯ (ç½®ä¿¡åº¦: {memory_context.confidence_score:.2f})")
            return enhanced_prompt
            
        except Exception as e:
            logger.warning(f"è®°å¿†å¢å¼ºæç¤ºè¯å¤±è´¥: {e}")
            return None
    
    def _format_memory_context(self, memory_context) -> str:
        """æ ¼å¼åŒ–è®°å¿†ä¸Šä¸‹æ–‡ä¸ºå¯è¯»æ–‡æœ¬"""
        sections = []
        
        if memory_context.semantic_memories:
            semantic_items = []
            for memory in memory_context.semantic_memories[:3]:  # å–æœ€ç›¸å…³çš„3ä¸ª
                semantic_items.append(f"- {memory.content[:100]}...")
            sections.append(f"ç›¸å…³è¯­ä¹‰è®°å¿†:\n" + "\n".join(semantic_items))
        
        if memory_context.episodic_memories:
            episodic_items = []
            for memory in memory_context.episodic_memories[:2]:  # å–æœ€ç›¸å…³çš„2ä¸ª
                episodic_items.append(f"- {memory.content[:100]}...")
            sections.append(f"ç›¸å…³æƒ…èŠ‚è®°å¿†:\n" + "\n".join(episodic_items))
        
        if memory_context.procedural_memories:
            procedural_items = []
            for memory in memory_context.procedural_memories[:2]:  # å–æœ€ç›¸å…³çš„2ä¸ª
                procedural_items.append(f"- {memory.content[:100]}...")
            sections.append(f"ç›¸å…³ç¨‹åºè®°å¿†:\n" + "\n".join(procedural_items))
        
        if memory_context.memory_summary:
            sections.append(f"è®°å¿†æ‘˜è¦: {memory_context.memory_summary}")
        
        return "\n\n".join(sections)
    
    def _build_base_prompt(self, user_query: str, state: IsotopeSystemState) -> str:
        """æ„å»ºåŸºç¡€æç¤ºè¯"""
        # è·å–æ–‡ä»¶ä¿¡æ¯
        available_files = state.get("files", [])
        file_list = []
        
        if isinstance(available_files, dict):
            for file_id, file_info in available_files.items():
                file_name = file_info.get("name", file_id)
                file_type = file_info.get("type", "æœªçŸ¥")
                file_size = file_info.get("size", 0)
                file_list.append(f"  - {file_name} (ID: {file_id}, ç±»å‹: {file_type}, å¤§å°: {file_size})")
        elif isinstance(available_files, list):
            for file_info in available_files:
                file_id = file_info.get("file_id", "æœªçŸ¥")
                file_name = file_info.get("name", file_id)
                file_type = file_info.get("type", "æœªçŸ¥")
                file_size = file_info.get("size", 0)
                file_list.append(f"  - {file_name} (ID: {file_id}, ç±»å‹: {file_type}, å¤§å°: {file_size})")
        
        # æ„å»ºä»»åŠ¡æè¿°
        available_tools_desc = []
        for tool_name, tool_func in self._available_tasks.items():
            tool_desc = getattr(tool_func, "__doc__", "").strip() if hasattr(tool_func, "__doc__") else ""
            if not tool_desc:
                tool_desc = f"æ‰§è¡Œ{tool_name}å·¥å…·"
            available_tools_desc.append(f"  - {tool_name}: {tool_desc}")
        
        return f"""ä½ æ˜¯{self.name}ï¼Œè§’è‰²æ˜¯{self.role}ã€‚

ç”¨æˆ·è¯·æ±‚: {user_query}

å¯ç”¨æ–‡ä»¶:
{chr(10).join(file_list) if file_list else "- æš‚æ— å¯ç”¨æ–‡ä»¶"}

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»»åŠ¡:
{chr(10).join(available_tools_desc)}

è¯·åˆ†æç”¨æˆ·è¯·æ±‚ï¼Œå†³å®šéœ€è¦æ‰§è¡Œå“ªäº›ä»»åŠ¡ã€‚å¦‚æœä»»åŠ¡éœ€è¦æ–‡ä»¶IDå‚æ•°ï¼Œè¯·ä½¿ç”¨ä¸Šé¢åˆ—å‡ºçš„å®é™…æ–‡ä»¶IDã€‚

**é‡è¦ï¼šå¿…é¡»ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡æœ¬ï¼š**

```json
{{
    "tasks_to_execute": [
        {{"task_name": "ä»»åŠ¡åç§°", "parameters": {{"file_id": "å®é™…çš„æ–‡ä»¶ID"}}}},
        {{"task_name": "å¦ä¸€ä¸ªä»»åŠ¡åç§°", "parameters": {{}}}}
    ],
    "reasoning": "ä½ çš„åˆ†æå’Œæ¨ç†è¿‡ç¨‹"
}}
```

æ³¨æ„ï¼š
1. åªè¾“å‡ºJSONï¼Œä¸è¦æœ‰å…¶ä»–è§£é‡Šæ–‡å­—
2. task_nameå¿…é¡»ä»ä¸Šé¢çš„å¯ç”¨ä»»åŠ¡åˆ—è¡¨ä¸­é€‰æ‹©
3. å¦‚æœéœ€è¦file_idå‚æ•°ï¼Œè¯·ä½¿ç”¨å®é™…çš„æ–‡ä»¶IDï¼Œä¸è¦ä½¿ç”¨å ä½ç¬¦
4. å¦‚æœä¸éœ€è¦æ‰§è¡Œä»»ä½•ä»»åŠ¡ï¼Œtasks_to_executeè®¾ä¸ºç©ºæ•°ç»„[]"""
    
    def _save_analysis_to_memory(self, state: IsotopeSystemState, analysis: Dict[str, Any]):
        """ä¿å­˜åˆ†æç»“æœåˆ°è®°å¿†ç³»ç»Ÿ"""
        try:
            # è·å–ç”¨æˆ·ä¿¡æ¯
            user_id = state.get('metadata', {}).get('user_id', 'default_user')
            session_id = state.get('metadata', {}).get('session_id', 'default_session')
            
            # æ„å»ºè®°å¿†å†…å®¹
            memory_content = f"æ™ºèƒ½ä½“ {self.name} åˆ†æç»“æœ:\n"
            memory_content += f"æ¨ç†è¿‡ç¨‹: {analysis.get('reasoning', '')}\n"
            memory_content += f"è¯†åˆ«ä»»åŠ¡: {[task.get('task_name') for task in analysis.get('tasks_to_execute', [])]}"
            
            # ä¿å­˜åˆ°è®°å¿†ç³»ç»Ÿ
            self.memory_integration.save_agent_interaction_memory(
                state=state,
                agent_role=self.role,
                interaction_summary=memory_content,
                session_id=session_id
            )
            
            logger.debug(f"æ™ºèƒ½ä½“ {self.name} åˆ†æç»“æœå·²ä¿å­˜åˆ°è®°å¿†ç³»ç»Ÿ")
            
        except Exception as e:
            logger.warning(f"ä¿å­˜åˆ†æç»“æœåˆ°è®°å¿†ç³»ç»Ÿå¤±è´¥: {e}")


def create_langgraph_agent(
    name: str,
    role: str,
    llm: Any,
    capabilities: Optional[List[str]] = None,
    config: Optional[Dict[str, Any]] = None
) -> LangGraphAgent:
    """
    å·¥å‚å‡½æ•°ï¼šåˆ›å»ºLangGraphæ™ºèƒ½ä½“
    
    Args:
        name: æ™ºèƒ½ä½“åç§°
        role: æ™ºèƒ½ä½“è§’è‰²
        llm: è¯­è¨€æ¨¡å‹å®ä¾‹
        capabilities: èƒ½åŠ›åˆ—è¡¨
        config: é…ç½®å‚æ•°
        
    Returns:
        LangGraphAgentå®ä¾‹
    """
    return LangGraphAgent(
        name=name,
        role=role,
        llm=llm,
        capabilities=capabilities,
        config=config
    ) 